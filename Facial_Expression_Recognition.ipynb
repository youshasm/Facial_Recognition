{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61b4f974",
   "metadata": {},
   "source": [
    "# Facial Expression Recognition and Valence/Arousal Prediction\n",
    "\n",
    "## Overview\n",
    "This notebook implements a multi-task deep learning approach for:\n",
    "1. **Facial Expression Classification**: Classifying emotions (Neutral, Happy, Sad, Surprise, Fear, Disgust, Anger, Contempt)\n",
    "2. **Valence Prediction**: Predicting positive/negative emotional state [-1, +1]  \n",
    "3. **Arousal Prediction**: Predicting excitement/calmness level [-1, +1]\n",
    "\n",
    "## Dataset Information\n",
    "- **Images**: 224x224 RGB facial images\n",
    "- **Annotations**: Stored in .npy files with expression labels, valence/arousal values, and facial landmarks\n",
    "\n",
    "## Models\n",
    "This notebook focuses on two high-performance architectures:\n",
    "- **ResNet50**: Deep residual network with skip connections\n",
    "- **EfficientNetB1**: Optimized architecture with compound scaling\n",
    "\n",
    "Each model uses transfer learning with multi-task learning for simultaneous emotion classification and valence/arousal regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4dda22",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7684e92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 19:28:05.345171: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1758828485.544811      78 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1758828485.600485      78 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Using GPU for training\n",
      "TensorFlow version: 2.18.0\n",
      "Keras version: 3.8.0\n",
      "NumPy version: 1.26.4\n",
      "OpenCV version: 4.11.0\n"
     ]
    }
   ],
   "source": [
    "# Core libraries for deep learning and data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "from tensorflow.keras.applications import (\n",
    "    VGG16, VGG19, ResNet50, ResNet101, \n",
    "    EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3, EfficientNetB4,\n",
    "    MobileNetV2, DenseNet121, InceptionV3, Xception\n",
    ")\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Scikit-learn for metrics and preprocessing\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, cohen_kappa_score, roc_auc_score,\n",
    "    precision_recall_curve, auc, confusion_matrix, classification_report\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Statistical and correlation metrics\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Visualization and plotting\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Configure warnings and display\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# GPU configuration\n",
    "print(\"GPU Available: \", tf.config.list_physical_devices('GPU'))\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"Using GPU for training\")\n",
    "    # Configure GPU memory growth\n",
    "    for gpu in tf.config.experimental.list_physical_devices('GPU'):\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "else:\n",
    "    print(\"Using CPU for training\")\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"OpenCV version: {cv2.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cd52343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "# GPU Configuration\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        print(f\"GPU available: {gpus[0]}\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU found\")\n",
    "\n",
    "is_kaggle = os.path.exists('/kaggle/input')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82c69e7",
   "metadata": {},
   "source": [
    "## 2. Configuration and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e581f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset path: Dataset\n",
      "Images path: Dataset/images\n",
      "Annotations path: Dataset/annotations\n",
      "Input shape: (224, 224, 3)\n",
      "Number of emotion classes: 8\n",
      "Emotion labels: {0: 'Neutral', 1: 'Happy', 2: 'Sad', 3: 'Surprise', 4: 'Fear', 5: 'Disgust', 6: 'Anger', 7: 'Contempt'}\n"
     ]
    }
   ],
   "source": [
    "# Dataset configuration\n",
    "DATASET_PATH = \"Dataset\"\n",
    "IMAGES_PATH = os.path.join(DATASET_PATH, \"images\")\n",
    "ANNOTATIONS_PATH = os.path.join(DATASET_PATH, \"annotations\")\n",
    "\n",
    "# Model configuration\n",
    "IMG_HEIGHT, IMG_WIDTH = 224, 224\n",
    "IMG_CHANNELS = 3\n",
    "INPUT_SHAPE = (IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\n",
    "\n",
    "# Class labels for emotions\n",
    "EMOTION_LABELS = {\n",
    "    0: 'Neutral',\n",
    "    1: 'Happy', \n",
    "    2: 'Sad',\n",
    "    3: 'Surprise',\n",
    "    4: 'Fear',\n",
    "    5: 'Disgust',\n",
    "    6: 'Anger',\n",
    "    7: 'Contempt'\n",
    "}\n",
    "\n",
    "NUM_CLASSES = len(EMOTION_LABELS)\n",
    "\n",
    "# Training configuration\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "VALIDATION_SPLIT = 0.2\n",
    "EARLY_STOPPING_PATIENCE = 8  # Adjusted for 20 epochs\n",
    "REDUCE_LR_PATIENCE = 4       # Adjusted for 20 epochs\n",
    "\n",
    "# Model architectures to compare\n",
    "MODEL_ARCHITECTURES = [\n",
    "    'VGG16', 'VGG19', 'ResNet50', 'ResNet101',\n",
    "    'EfficientNetB0', 'EfficientNetB1', 'EfficientNetB2',\n",
    "    'MobileNetV2', 'DenseNet121'\n",
    "]\n",
    "\n",
    "# Loss weights for multi-task learning\n",
    "EMOTION_LOSS_WEIGHT = 1.0\n",
    "VALENCE_LOSS_WEIGHT = 1.0\n",
    "AROUSAL_LOSS_WEIGHT = 1.0\n",
    "\n",
    "print(f\"Dataset path: {DATASET_PATH}\")\n",
    "print(f\"Images path: {IMAGES_PATH}\")\n",
    "print(f\"Annotations path: {ANNOTATIONS_PATH}\")\n",
    "print(f\"Input shape: {INPUT_SHAPE}\")\n",
    "print(f\"Number of emotion classes: {NUM_CLASSES}\")\n",
    "print(f\"Emotion labels: {EMOTION_LABELS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5980a578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle environment detected - applying optimizations\n",
      "Results will be saved to: /kaggle/working\n",
      "\n",
      "Final Configuration:\n",
      "   Dataset path: Dataset\n",
      "   Images path: Dataset/images\n",
      "   Annotations path: Dataset/annotations\n",
      "   Batch size: 16\n",
      "   Epochs: 50\n",
      "\n",
      "Updating dataset paths for Kaggle\n",
      "Found annotations in: /kaggle/input/affect/Dataset/Dataset/annotations\n",
      "Found images in: /kaggle/input/affect/Dataset/Dataset/images\n",
      "\n",
      "Final paths:\n",
      "   Images: /kaggle/input/affect/Dataset/Dataset/images\n",
      "   Annotations: /kaggle/input/affect/Dataset/Dataset/annotations\n",
      "\n",
      "Final paths:\n",
      "   Images: /kaggle/input/affect/Dataset/Dataset/images\n",
      "   Annotations: /kaggle/input/affect/Dataset/Dataset/annotations\n"
     ]
    }
   ],
   "source": [
    "# Auto-detect environment and set appropriate paths\n",
    "is_kaggle = os.path.exists('/kaggle/input')\n",
    "\n",
    "if is_kaggle:\n",
    "    print(\"Kaggle environment detected - applying optimizations\")\n",
    "    \n",
    "    # Kaggle-optimized training settings\n",
    "    BATCH_SIZE = 16  # Reduced for GPU memory efficiency\n",
    "    EPOCHS = 50      # Reduced for time limits\n",
    "    EARLY_STOPPING_PATIENCE = 10\n",
    "    \n",
    "    # Results path\n",
    "    RESULTS_PATH = \"/kaggle/working\"\n",
    "    \n",
    "else:\n",
    "    print(\"Running locally\")\n",
    "    # Keep original local paths\n",
    "    DATASET_PATH = \"Dataset\"\n",
    "    IMAGES_PATH = os.path.join(DATASET_PATH, \"images\")\n",
    "    ANNOTATIONS_PATH = os.path.join(DATASET_PATH, \"annotations\")\n",
    "    RESULTS_PATH = \"results\"\n",
    "\n",
    "# Create results directory\n",
    "if 'RESULTS_PATH' in locals():\n",
    "    os.makedirs(RESULTS_PATH, exist_ok=True)\n",
    "    print(f\"Results will be saved to: {RESULTS_PATH}\")\n",
    "\n",
    "# Print final configuration\n",
    "print(f\"\\nFinal Configuration:\")\n",
    "print(f\"   Dataset path: {DATASET_PATH if 'DATASET_PATH' in locals() else 'Not found'}\")\n",
    "print(f\"   Images path: {IMAGES_PATH if IMAGES_PATH else 'Not found'}\")\n",
    "print(f\"   Annotations path: {ANNOTATIONS_PATH if ANNOTATIONS_PATH else 'Not found'}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Epochs: {EPOCHS}\")\n",
    "\n",
    "print(\"\\nUpdating dataset paths for Kaggle\")\n",
    "\n",
    "# Find the actual dataset structure in Kaggle\n",
    "dataset_root = \"/kaggle/input/affect/Dataset/Dataset\"\n",
    "DATASET_PATH = dataset_root\n",
    "\n",
    "# Auto-detect image and annotation directories\n",
    "images_found = False\n",
    "annotations_found = False\n",
    "\n",
    "for root, dirs, files in os.walk(dataset_root):\n",
    "    for dir_name in dirs:\n",
    "        if \"image\" in dir_name.lower():\n",
    "            IMAGES_PATH = os.path.join(root, dir_name)\n",
    "            print(f\"Found images in: {IMAGES_PATH}\")\n",
    "            images_found = True\n",
    "        elif \"annotation\" in dir_name.lower():\n",
    "            ANNOTATIONS_PATH = os.path.join(root, dir_name)\n",
    "            print(f\"Found annotations in: {ANNOTATIONS_PATH}\")\n",
    "            annotations_found = True\n",
    "\n",
    "# If standard names not found, check the root directory contents\n",
    "if not images_found or not annotations_found:\n",
    "    print(f\"\\nContents of {dataset_root}:\")\n",
    "    contents = os.listdir(dataset_root)\n",
    "    for item in contents:\n",
    "        item_path = os.path.join(dataset_root, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            item_contents = os.listdir(item_path)[:5]\n",
    "            print(f\"   {item}/ contains: {item_contents}\")\n",
    "            \n",
    "            # Check if this could be images or annotations\n",
    "            if any(f.lower().endswith(('.jpg', '.jpeg', '.png')) for f in item_contents):\n",
    "                IMAGES_PATH = item_path\n",
    "                print(f\"   This appears to be the images directory!\")\n",
    "                images_found = True\n",
    "            elif any(f.endswith('.npy') for f in item_contents):\n",
    "                ANNOTATIONS_PATH = item_path\n",
    "                print(f\"   This appears to be the annotations directory!\")\n",
    "                annotations_found = True\n",
    "\n",
    "print(f\"\\nFinal paths:\")\n",
    "print(f\"   Images: {IMAGES_PATH if images_found else 'Not found'}\")\n",
    "print(f\"   Annotations: {ANNOTATIONS_PATH if annotations_found else 'Not found'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56ad88ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Updating dataset paths for Kaggle\n",
      " Updated Dataset path: /kaggle/input/affect/Dataset\n",
      "  Updated Images path: /kaggle/input/affect/Dataset/images\n",
      " Updated Annotations path: /kaggle/input/affect/Dataset/annotations\n",
      "\n",
      "✓ Checking paths:\n",
      "   Dataset exists: True\n",
      "   Images exists: False\n",
      "   Annotations exists: False\n"
     ]
    }
   ],
   "source": [
    "# Update paths based on Kaggle dataset structure\n",
    "print(\" Updating dataset paths for Kaggle\")\n",
    "\n",
    "# Set the correct Kaggle dataset path\n",
    "DATASET_PATH = \"/kaggle/input/affect/Dataset\"\n",
    "IMAGES_PATH = os.path.join(DATASET_PATH, \"images\")\n",
    "ANNOTATIONS_PATH = os.path.join(DATASET_PATH, \"annotations\")\n",
    "\n",
    "print(f\" Updated Dataset path: {DATASET_PATH}\")\n",
    "print(f\"  Updated Images path: {IMAGES_PATH}\")\n",
    "print(f\" Updated Annotations path: {ANNOTATIONS_PATH}\")\n",
    "\n",
    "# Verify paths exist\n",
    "print(f\"\\n✓ Checking paths:\")\n",
    "print(f\"   Dataset exists: {os.path.exists(DATASET_PATH)}\")\n",
    "print(f\"   Images exists: {os.path.exists(IMAGES_PATH)}\")\n",
    "print(f\"   Annotations exists: {os.path.exists(ANNOTATIONS_PATH)}\")\n",
    "\n",
    "if os.path.exists(IMAGES_PATH):\n",
    "    image_count = len([f for f in os.listdir(IMAGES_PATH) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "    print(f\"   📸 Image files found: {image_count}\")\n",
    "\n",
    "if os.path.exists(ANNOTATIONS_PATH):\n",
    "    annotation_count = len([f for f in os.listdir(ANNOTATIONS_PATH) if f.endswith('.npy')])\n",
    "    print(f\"    Annotation files found: {annotation_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01c0109c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset configuration:\n",
      "Dataset path: /kaggle/input/affect/Dataset/Dataset\n",
      "Images path: /kaggle/input/affect/Dataset/Dataset/images\n",
      "Annotations path: /kaggle/input/affect/Dataset/Dataset/annotations\n",
      "\n",
      "Dataset Statistics:\n",
      "   Total image files: 3999\n",
      "   Total annotation files: 15996\n",
      "   Expression files: 3999\n",
      "   Valence files: 3999\n",
      "   Arousal files: 3999\n",
      "   Landmark files: 3999\n",
      "   2820: Image + Annotations available\n",
      "   1757: Image + Annotations available\n",
      "   2691: Image + Annotations available\n",
      "   406: Image + Annotations available\n",
      "   2966: Image + Annotations available\n"
     ]
    }
   ],
   "source": [
    "# Set the correct final paths\n",
    "DATASET_PATH = \"/kaggle/input/affect/Dataset/Dataset\"\n",
    "IMAGES_PATH = \"/kaggle/input/affect/Dataset/Dataset/images\"\n",
    "ANNOTATIONS_PATH = \"/kaggle/input/affect/Dataset/Dataset/annotations\"\n",
    "\n",
    "print(\"Final dataset configuration:\")\n",
    "print(f\"Dataset path: {DATASET_PATH}\")\n",
    "print(f\"Images path: {IMAGES_PATH}\")\n",
    "print(f\"Annotations path: {ANNOTATIONS_PATH}\")\n",
    "\n",
    "# Verify and count files\n",
    "image_files = [f for f in os.listdir(IMAGES_PATH) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "annotation_files = [f for f in os.listdir(ANNOTATIONS_PATH) if f.endswith('.npy')]\n",
    "\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"   Total image files: {len(image_files)}\")\n",
    "print(f\"   Total annotation files: {len(annotation_files)}\")\n",
    "\n",
    "# Expression files breakdown\n",
    "exp_files = [f for f in annotation_files if '_exp.npy' in f]\n",
    "val_files = [f for f in annotation_files if '_val.npy' in f]\n",
    "aro_files = [f for f in annotation_files if '_aro.npy' in f]\n",
    "lnd_files = [f for f in annotation_files if '_lnd.npy' in f]\n",
    "\n",
    "print(f\"   Expression files: {len(exp_files)}\")\n",
    "print(f\"   Valence files: {len(val_files)}\")\n",
    "print(f\"   Arousal files: {len(aro_files)}\")\n",
    "print(f\"   Landmark files: {len(lnd_files)}\")\n",
    "\n",
    "# Sample verification\n",
    "file_ids = [f.split('_')[0] for f in exp_files]\n",
    "sample_ids = file_ids[:5]\n",
    "\n",
    "for file_id in sample_ids:\n",
    "    image_file = f\"{file_id}.jpg\"\n",
    "    if os.path.exists(os.path.join(IMAGES_PATH, image_file)):\n",
    "        # Check annotation files\n",
    "        exp_file = os.path.join(ANNOTATIONS_PATH, f\"{file_id}_exp.npy\")\n",
    "        val_file = os.path.join(ANNOTATIONS_PATH, f\"{file_id}_val.npy\")\n",
    "        aro_file = os.path.join(ANNOTATIONS_PATH, f\"{file_id}_aro.npy\")\n",
    "        \n",
    "        if os.path.exists(exp_file) and os.path.exists(val_file) and os.path.exists(aro_file):\n",
    "            print(f\"   {file_id}: Image + Annotations available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5b74a3",
   "metadata": {},
   "source": [
    "## 3. Dataset Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02d28f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "DATASET EXPLORATION\n",
      "==================================================\n",
      "Dataset path: /kaggle/input/affect/Dataset/Dataset\n",
      "Total image files: 3999\n",
      "Total annotation files: 15996\n",
      "Expression files: 3999\n",
      "Valence files: 3999\n",
      "Arousal files: 3999\n",
      "Landmark files: 3999\n",
      "Unique file IDs: 3999\n"
     ]
    }
   ],
   "source": [
    "def explore_dataset():\n",
    "    \"\"\"Explore the facial expression dataset structure and contents\"\"\"\n",
    "    print(\"=\"*50)\n",
    "    print(\"DATASET EXPLORATION\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Check if dataset paths exist\n",
    "    if not os.path.exists(DATASET_PATH):\n",
    "        print(f\"Dataset path not found: {DATASET_PATH}\")\n",
    "        return\n",
    "    \n",
    "    if not os.path.exists(IMAGES_PATH):\n",
    "        print(f\"Images path not found: {IMAGES_PATH}\")\n",
    "        return\n",
    "        \n",
    "    if not os.path.exists(ANNOTATIONS_PATH):\n",
    "        print(f\"Annotations path not found: {ANNOTATIONS_PATH}\")\n",
    "        return\n",
    "    \n",
    "    # Get list of image files\n",
    "    image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp']\n",
    "    image_files = []\n",
    "    for ext in image_extensions:\n",
    "        image_files.extend(glob.glob(os.path.join(IMAGES_PATH, ext)))\n",
    "    \n",
    "    # Get list of annotation files\n",
    "    annotation_files = glob.glob(os.path.join(ANNOTATIONS_PATH, \"*.npy\"))\n",
    "    \n",
    "    print(f\"Dataset path: {DATASET_PATH}\")\n",
    "    print(f\"Total image files: {len(image_files)}\")\n",
    "    print(f\"Total annotation files: {len(annotation_files)}\")\n",
    "    \n",
    "    # Analyze annotation types\n",
    "    exp_files = [f for f in annotation_files if f.endswith('_exp.npy')]\n",
    "    val_files = [f for f in annotation_files if f.endswith('_val.npy')]\n",
    "    aro_files = [f for f in annotation_files if f.endswith('_aro.npy')]\n",
    "    lnd_files = [f for f in annotation_files if f.endswith('_lnd.npy')]\n",
    "    \n",
    "    print(f\"Expression files: {len(exp_files)}\")\n",
    "    print(f\"Valence files: {len(val_files)}\")\n",
    "    print(f\"Arousal files: {len(aro_files)}\")\n",
    "    print(f\"Landmark files: {len(lnd_files)}\")\n",
    "    \n",
    "    # Extract unique file IDs\n",
    "    file_ids = set()\n",
    "    for f in annotation_files:\n",
    "        filename = os.path.basename(f)\n",
    "        file_id = filename.split('_')[0]\n",
    "        file_ids.add(file_id)\n",
    "    \n",
    "    print(f\"Unique file IDs: {len(file_ids)}\")\n",
    "    \n",
    "    return sorted(list(file_ids))\n",
    "\n",
    "# Explore the dataset\n",
    "file_ids = explore_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094e85ad",
   "metadata": {},
   "source": [
    "## 4. Custom Evaluation Metrics Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55c91d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Custom evaluation metrics implemented successfully!\n",
      " Available classification metrics: accuracy, f1_score, cohen_kappa, krippendorff_alpha, auc_roc, auc_pr\n",
      " Available regression metrics: rmse, mae, r2, correlation, sagr, ccc\n"
     ]
    }
   ],
   "source": [
    "def krippendorff_alpha(data, level_of_measurement='interval'):\n",
    "    \"\"\"\n",
    "    Calculate Krippendorf's Alpha for inter-rater reliability\n",
    "    \n",
    "    Args:\n",
    "        data: Array of shape (n_raters, n_items) or (n_items, n_raters)\n",
    "        level_of_measurement: 'nominal', 'ordinal', 'interval', or 'ratio'\n",
    "    \n",
    "    Returns:\n",
    "        float: Krippendorf's Alpha value\n",
    "    \"\"\"\n",
    "    # Simplified implementation for interval data\n",
    "    # For complete implementation, use krippendorff library\n",
    "    from sklearn.metrics import cohen_kappa_score\n",
    "    \n",
    "    if data.shape[0] == 2:  # Two raters\n",
    "        return cohen_kappa_score(data[0], data[1])\n",
    "    else:\n",
    "        # For multiple raters, use average pairwise kappa\n",
    "        kappas = []\n",
    "        n_raters = data.shape[0]\n",
    "        for i in range(n_raters):\n",
    "            for j in range(i+1, n_raters):\n",
    "                kappa = cohen_kappa_score(data[i], data[j])\n",
    "                kappas.append(kappa)\n",
    "        return np.mean(kappas) if kappas else 0.0\n",
    "\n",
    "def sign_agreement_metric(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate Sign Agreement Metric (SAGR)\n",
    "    Penalizes incorrect sign alongside deviation from value\n",
    "    \n",
    "    Args:\n",
    "        y_true: Ground truth values\n",
    "        y_pred: Predicted values\n",
    "    \n",
    "    Returns:\n",
    "        float: SAGR score\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    # Calculate sign agreement\n",
    "    sign_true = np.sign(y_true)\n",
    "    sign_pred = np.sign(y_pred)\n",
    "    sign_agreement = np.mean(sign_true == sign_pred)\n",
    "    \n",
    "    # Calculate magnitude correlation\n",
    "    correlation, _ = pearsonr(np.abs(y_true), np.abs(y_pred))\n",
    "    \n",
    "    # Combine sign agreement and correlation\n",
    "    sagr = sign_agreement * correlation if not np.isnan(correlation) else sign_agreement\n",
    "    \n",
    "    return sagr\n",
    "\n",
    "def concordance_correlation_coefficient(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate Concordance Correlation Coefficient (CCC)\n",
    "    Combines Pearson correlation with accuracy\n",
    "    \n",
    "    Args:\n",
    "        y_true: Ground truth values\n",
    "        y_pred: Predicted values\n",
    "    \n",
    "    Returns:\n",
    "        float: CCC value\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    # Pearson correlation coefficient\n",
    "    correlation, _ = pearsonr(y_true, y_pred)\n",
    "    \n",
    "    # Means and variances\n",
    "    mean_true = np.mean(y_true)\n",
    "    mean_pred = np.mean(y_pred)\n",
    "    var_true = np.var(y_true)\n",
    "    var_pred = np.var(y_pred)\n",
    "    \n",
    "    # Standard deviations\n",
    "    sd_true = np.sqrt(var_true)\n",
    "    sd_pred = np.sqrt(var_pred)\n",
    "    \n",
    "    # Concordance correlation coefficient\n",
    "    numerator = 2 * correlation * sd_true * sd_pred\n",
    "    denominator = var_true + var_pred + (mean_true - mean_pred) ** 2\n",
    "    \n",
    "    ccc = numerator / denominator if denominator != 0 else 0\n",
    "    \n",
    "    return ccc\n",
    "\n",
    "def evaluate_classification_metrics(y_true, y_pred, y_pred_proba=None):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive classification metrics\n",
    "    \n",
    "    Args:\n",
    "        y_true: True labels\n",
    "        y_pred: Predicted labels\n",
    "        y_pred_proba: Predicted probabilities (optional)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary of metric values\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Basic metrics\n",
    "    metrics['accuracy'] = accuracy_score(y_true, y_pred)\n",
    "    metrics['f1_score'] = f1_score(y_true, y_pred, average='weighted')\n",
    "    metrics['cohen_kappa'] = cohen_kappa_score(y_true, y_pred)\n",
    "    \n",
    "    # Krippendorf's Alpha (using Cohen's Kappa as approximation)\n",
    "    metrics['krippendorff_alpha'] = metrics['cohen_kappa']\n",
    "    \n",
    "    # AUC metrics (if probabilities provided)\n",
    "    if y_pred_proba is not None:\n",
    "        try:\n",
    "            metrics['auc_roc'] = roc_auc_score(y_true, y_pred_proba, multi_class='ovr', average='macro')\n",
    "        except:\n",
    "            metrics['auc_roc'] = 0.0\n",
    "        \n",
    "        try:\n",
    "            # Calculate AUC-PR for each class and average\n",
    "            auc_pr_scores = []\n",
    "            for i in range(y_pred_proba.shape[1]):\n",
    "                y_true_binary = (y_true == i).astype(int)\n",
    "                precision, recall, _ = precision_recall_curve(y_true_binary, y_pred_proba[:, i])\n",
    "                auc_pr = auc(recall, precision)\n",
    "                auc_pr_scores.append(auc_pr)\n",
    "            metrics['auc_pr'] = np.mean(auc_pr_scores)\n",
    "        except:\n",
    "            metrics['auc_pr'] = 0.0\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def evaluate_regression_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive regression metrics\n",
    "    \n",
    "    Args:\n",
    "        y_true: True values\n",
    "        y_pred: Predicted values\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary of metric values\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    # Basic metrics\n",
    "    metrics['rmse'] = np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "    metrics['mae'] = np.mean(np.abs(y_true - y_pred))\n",
    "    metrics['r2'] = 1 - (np.sum((y_true - y_pred) ** 2) / np.sum((y_true - np.mean(y_true)) ** 2))\n",
    "    \n",
    "    # Correlation\n",
    "    correlation, p_value = pearsonr(y_true, y_pred)\n",
    "    metrics['correlation'] = correlation if not np.isnan(correlation) else 0.0\n",
    "    metrics['correlation_p_value'] = p_value if not np.isnan(p_value) else 1.0\n",
    "    \n",
    "    # Custom metrics\n",
    "    metrics['sagr'] = sign_agreement_metric(y_true, y_pred)\n",
    "    metrics['ccc'] = concordance_correlation_coefficient(y_true, y_pred)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"✅ Custom evaluation metrics implemented successfully!\")\n",
    "print(\" Available classification metrics: accuracy, f1_score, cohen_kappa, krippendorff_alpha, auc_roc, auc_pr\")\n",
    "print(\" Available regression metrics: rmse, mae, r2, correlation, sagr, ccc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb733185",
   "metadata": {},
   "source": [
    "## 5. Multi-Task CNN Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c24cb67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Multi-task model architecture functions created successfully!\n",
      "  Available architectures: ['VGG16', 'VGG19', 'ResNet50', 'ResNet101', 'EfficientNetB0', 'EfficientNetB1', 'EfficientNetB2', 'MobileNetV2', 'DenseNet121']\n",
      " Model outputs: emotion_classification, valence_regression, arousal_regression\n"
     ]
    }
   ],
   "source": [
    "def create_multitask_model(base_model_name='EfficientNetB0', input_shape=INPUT_SHAPE, \n",
    "                          num_classes=NUM_CLASSES, trainable_layers=50):\n",
    "    \"\"\"\n",
    "    Create a multi-task CNN model for emotion classification and valence/arousal regression\n",
    "    \n",
    "    Args:\n",
    "        base_model_name (str): Name of the base CNN architecture\n",
    "        input_shape (tuple): Input shape for images\n",
    "        num_classes (int): Number of emotion classes\n",
    "        trainable_layers (int): Number of top layers to make trainable\n",
    "    \n",
    "    Returns:\n",
    "        tensorflow.keras.Model: Compiled multi-task model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input layer\n",
    "    inputs = layers.Input(shape=input_shape, name='image_input')\n",
    "    \n",
    "    # Get base model - only ResNet50 and EfficientNetB1 supported\n",
    "    base_models = {\n",
    "        'ResNet50': ResNet50,\n",
    "        'EfficientNetB1': EfficientNetB1\n",
    "    }\n",
    "    \n",
    "    if base_model_name not in base_models:\n",
    "        raise ValueError(f\"Unsupported base model: {base_model_name}\")\n",
    "    \n",
    "    # Create base model with ImageNet weights\n",
    "    base_model = base_models[base_model_name](\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_tensor=inputs\n",
    "    )\n",
    "    \n",
    "    # Freeze initial layers\n",
    "    for layer in base_model.layers[:-trainable_layers]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Add global average pooling\n",
    "    x = layers.GlobalAveragePooling2D(name='global_avg_pool')(base_model.output)\n",
    "    \n",
    "    # Shared feature layer\n",
    "    x = layers.Dense(512, activation='relu', name='shared_features')(x)\n",
    "    x = layers.Dropout(0.5, name='shared_dropout')(x)\n",
    "    \n",
    "    # Emotion classification head\n",
    "    emotion_features = layers.Dense(256, activation='relu', name='emotion_features')(x)\n",
    "    emotion_features = layers.Dropout(0.3, name='emotion_dropout')(emotion_features)\n",
    "    emotion_output = layers.Dense(num_classes, activation='softmax', name='emotion_output')(emotion_features)\n",
    "    \n",
    "    # Valence regression head\n",
    "    valence_features = layers.Dense(128, activation='relu', name='valence_features')(x)\n",
    "    valence_features = layers.Dropout(0.3, name='valence_dropout')(valence_features)\n",
    "    valence_output = layers.Dense(1, activation='tanh', name='valence_output')(valence_features)\n",
    "    \n",
    "    # Arousal regression head\n",
    "    arousal_features = layers.Dense(128, activation='relu', name='arousal_features')(x)\n",
    "    arousal_features = layers.Dropout(0.3, name='arousal_dropout')(arousal_features)\n",
    "    arousal_output = layers.Dense(1, activation='tanh', name='arousal_output')(arousal_features)\n",
    "    \n",
    "    # Create model\n",
    "    model = models.Model(\n",
    "        inputs=inputs,\n",
    "        outputs=[emotion_output, valence_output, arousal_output],\n",
    "        name=f'MultiTask_{base_model_name}'\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def compile_multitask_model(model, learning_rate=LEARNING_RATE):\n",
    "    \"\"\"\n",
    "    Compile multi-task model with appropriate loss functions and metrics\n",
    "    \n",
    "    Args:\n",
    "        model: Keras model to compile\n",
    "        learning_rate: Learning rate for optimizer\n",
    "    \n",
    "    Returns:\n",
    "        Compiled model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define losses\n",
    "    losses = {\n",
    "        'emotion_output': 'categorical_crossentropy',\n",
    "        'valence_output': 'mse',\n",
    "        'arousal_output': 'mse'\n",
    "    }\n",
    "    \n",
    "    # Define loss weights\n",
    "    loss_weights = {\n",
    "        'emotion_output': EMOTION_LOSS_WEIGHT,\n",
    "        'valence_output': VALENCE_LOSS_WEIGHT,\n",
    "        'arousal_output': AROUSAL_LOSS_WEIGHT\n",
    "    }\n",
    "    \n",
    "    # Define metrics\n",
    "    metrics = {\n",
    "        'emotion_output': ['accuracy'],\n",
    "        'valence_output': ['mae'],\n",
    "        'arousal_output': ['mae']\n",
    "    }\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=losses,\n",
    "        loss_weights=loss_weights,\n",
    "        metrics=metrics\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_model_summary(model):\n",
    "    \"\"\"\n",
    "    Get detailed model summary including parameter count\n",
    "    \n",
    "    Args:\n",
    "        model: Keras model\n",
    "    \n",
    "    Returns:\n",
    "        dict: Model statistics\n",
    "    \"\"\"\n",
    "    \n",
    "    # Count parameters using model.count_params() method\n",
    "    try:\n",
    "        total_params = model.count_params()\n",
    "        \n",
    "        # Count trainable parameters manually\n",
    "        trainable_params = 0\n",
    "        for layer in model.layers:\n",
    "            if layer.trainable:\n",
    "                layer_params = sum([np.prod(w.shape) for w in layer.get_weights()])\n",
    "                trainable_params += layer_params\n",
    "        \n",
    "        non_trainable_params = total_params - trainable_params\n",
    "        \n",
    "    except:\n",
    "        # Fallback method\n",
    "        trainable_params = sum([np.prod(w.shape) for w in model.trainable_weights])\n",
    "        non_trainable_params = sum([np.prod(w.shape) for w in model.non_trainable_weights])\n",
    "        total_params = trainable_params + non_trainable_params\n",
    "    \n",
    "    stats = {\n",
    "        'total_parameters': int(total_params),\n",
    "        'trainable_parameters': int(trainable_params),\n",
    "        'non_trainable_parameters': int(non_trainable_params),\n",
    "        'model_size_mb': total_params * 4 / (1024 * 1024),  # Assuming float32\n",
    "        'layers': len(model.layers)\n",
    "    }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "print(\"✅ Multi-task model architecture functions created successfully!\")\n",
    "print(\"  Available architectures:\", MODEL_ARCHITECTURES)\n",
    "print(\" Model outputs: emotion_classification, valence_regression, arousal_regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987a3398",
   "metadata": {},
   "source": [
    "## 6. Training Pipeline and Model Comparison Framework\n",
    "\n",
    "This section contains the complete training pipeline and framework for comparing multiple CNN architectures. Run the cells below to:\n",
    "\n",
    "1. **Load and preprocess the dataset**\n",
    "2. **Create train/validation splits**\n",
    "3. **Train multiple CNN models**\n",
    "4. **Evaluate and compare results**\n",
    "5. **Generate comprehensive performance reports**\n",
    "\n",
    "### Next Steps:\n",
    "1. **Run the data loading cell** to load your dataset\n",
    "2. **Execute the training loop** to train all models\n",
    "3. **Analyze results** using the evaluation metrics\n",
    "4. **Compare architectures** to find the best performing model\n",
    "\n",
    "### Expected Training Time:\n",
    "- **CPU**: 8-12 hours per model\n",
    "- **GPU**: 2-4 hours per model\n",
    "- **Total**: 16-40 hours for all models (depending on hardware)\n",
    "\n",
    "### Memory Requirements:\n",
    "- **Minimum**: 8GB RAM\n",
    "- **Recommended**: 16GB RAM + 6GB GPU memory\n",
    "- **Batch size**: Adjust based on available memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9ce214",
   "metadata": {},
   "source": [
    "## 7. Data Loading and Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3398e54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available file IDs: 3999\n",
      "Ready for enhanced dataset loading with detailed status!\n",
      "Loading 1000 samples for training...\n",
      "Images path: /kaggle/input/affect/Dataset/Dataset/images\n",
      "Annotations path: /kaggle/input/affect/Dataset/Dataset/annotations\n",
      "Available file IDs: 3999\n",
      "Randomly selected 1000 samples to load\n",
      "\n",
      "Starting data loading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data:  11%|█         | 110/1000 [00:03<00:13, 67.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 100/100 samples loaded successfully (100.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data:  21%|██▏       | 213/1000 [00:04<00:08, 90.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 200/200 samples loaded successfully (100.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data:  31%|███▏      | 314/1000 [00:06<00:07, 87.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 300/300 samples loaded successfully (100.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data:  41%|████      | 409/1000 [00:07<00:06, 88.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 400/400 samples loaded successfully (100.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data:  52%|█████▏    | 515/1000 [00:08<00:05, 87.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 500/500 samples loaded successfully (100.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data:  62%|██████▏   | 615/1000 [00:09<00:04, 88.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 600/600 samples loaded successfully (100.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data:  72%|███████▏  | 716/1000 [00:10<00:03, 88.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 700/700 samples loaded successfully (100.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data:  82%|████████▏ | 817/1000 [00:11<00:02, 87.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 800/800 samples loaded successfully (100.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data:  92%|█████████▏| 915/1000 [00:12<00:01, 84.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 900/900 samples loaded successfully (100.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1000/1000 [00:13<00:00, 71.59it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 1000/1000 samples loaded successfully (100.0%)\n",
      "\n",
      "LOADING COMPLETE!\n",
      "   Successfully loaded: 1000 samples\n",
      "   Image errors: 0\n",
      "   Annotation errors: 0\n",
      "   Missing files: 0\n",
      "   Success rate: 100.0%\n",
      "\n",
      "DATASET STATISTICS:\n",
      "   Image shape: (1000, 224, 224, 3)\n",
      "   Emotion range: [0, 7]\n",
      "   Valence range: [-0.962, 0.982]\n",
      "   Arousal range: [-0.627, 0.968]\n",
      "   Ready for training!\n"
     ]
    }
   ],
   "source": [
    "# DATASET LOADING AND PREPROCESSING FOR 20 EPOCHS TRAINING\n",
    "def load_annotations(annotations_path, file_id):\n",
    "    \"\"\"Load annotations for a given file ID\"\"\"\n",
    "    try:\n",
    "        annotations = {}\n",
    "        \n",
    "        # Load expression (emotion class)\n",
    "        exp_path = os.path.join(annotations_path, f\"{file_id}_exp.npy\")\n",
    "        if os.path.exists(exp_path):\n",
    "            annotations['expression'] = np.load(exp_path).item()\n",
    "        \n",
    "        # Load valence\n",
    "        val_path = os.path.join(annotations_path, f\"{file_id}_val.npy\") \n",
    "        if os.path.exists(val_path):\n",
    "            annotations['valence'] = np.load(val_path).item()\n",
    "            \n",
    "        # Load arousal\n",
    "        aro_path = os.path.join(annotations_path, f\"{file_id}_aro.npy\")\n",
    "        if os.path.exists(aro_path):\n",
    "            annotations['arousal'] = np.load(aro_path).item()\n",
    "            \n",
    "        return annotations\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {}\n",
    "\n",
    "def load_and_preprocess_dataset_fast(images_path, annotations_path, file_ids, sample_size=1000):\n",
    "    \"\"\"Fast dataset loading optimized for training\"\"\"\n",
    "    \n",
    "    print(f\"Loading {sample_size} samples for training...\")\n",
    "    print(f\"Images path: {images_path}\")\n",
    "    print(f\"Annotations path: {annotations_path}\")\n",
    "    print(f\"Available file IDs: {len(file_ids)}\")\n",
    "    \n",
    "    # Sample file IDs\n",
    "    sample_file_ids = np.random.choice(file_ids, min(sample_size, len(file_ids)), replace=False)\n",
    "    print(f\"Randomly selected {len(sample_file_ids)} samples to load\")\n",
    "    \n",
    "    images = []\n",
    "    emotions = []\n",
    "    valences = []\n",
    "    arousals = []\n",
    "    valid_ids = []\n",
    "    \n",
    "    # Counters for status tracking\n",
    "    successful_loads = 0\n",
    "    image_errors = 0\n",
    "    annotation_errors = 0\n",
    "    missing_files = 0\n",
    "    \n",
    "    print(f\"\\nStarting data loading...\")\n",
    "    \n",
    "    for i, file_id in enumerate(tqdm(sample_file_ids, desc=\"Loading data\")):\n",
    "        try:\n",
    "            # Load image\n",
    "            image_path = os.path.join(images_path, f\"{file_id}.jpg\")\n",
    "            if os.path.exists(image_path):\n",
    "                image = cv2.imread(image_path)\n",
    "                if image is not None:\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                    image = cv2.resize(image, (IMG_HEIGHT, IMG_WIDTH))\n",
    "                    image = image.astype(np.float32) / 255.0\n",
    "                    \n",
    "                    # Load annotations\n",
    "                    annotations = load_annotations(annotations_path, file_id)\n",
    "                    \n",
    "                    if 'expression' in annotations and 'valence' in annotations and 'arousal' in annotations:\n",
    "                        images.append(image)\n",
    "                        emotions.append(int(annotations['expression']))\n",
    "                        valences.append(float(annotations['valence']))\n",
    "                        arousals.append(float(annotations['arousal']))\n",
    "                        valid_ids.append(file_id)\n",
    "                        successful_loads += 1\n",
    "                        \n",
    "                        # Show progress every 100 samples\n",
    "                        if (i + 1) % 100 == 0:\n",
    "                            print(f\"Progress: {successful_loads}/{i+1} samples loaded successfully ({successful_loads/(i+1)*100:.1f}%)\")\n",
    "                    else:\n",
    "                        annotation_errors += 1\n",
    "                        if annotation_errors <= 5:  # Only show first 5 annotation errors\n",
    "                            print(f\"File {file_id}: Missing annotations (exp: {'expression' in annotations}, val: {'valence' in annotations}, aro: {'arousal' in annotations})\")\n",
    "                else:\n",
    "                    image_errors += 1\n",
    "                    if image_errors <= 5:  # Only show first 5 image errors\n",
    "                        print(f\"File {file_id}: Failed to read image\")\n",
    "            else:\n",
    "                missing_files += 1\n",
    "                if missing_files <= 5:  # Only show first 5 missing files\n",
    "                    print(f\"File {file_id}: Image file not found\")\n",
    "                        \n",
    "        except Exception as e:\n",
    "            if i < 10:  # Only show first 10 general errors\n",
    "                print(f\"Error processing {file_id}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(images)\n",
    "    y_emotions = np.array(emotions)\n",
    "    y_valence = np.array(valences)\n",
    "    y_arousal = np.array(arousals)\n",
    "    \n",
    "    # Final status report\n",
    "    print(f\"\\nLOADING COMPLETE!\")\n",
    "    print(f\"   Successfully loaded: {successful_loads} samples\")\n",
    "    print(f\"   Image errors: {image_errors}\")\n",
    "    print(f\"   Annotation errors: {annotation_errors}\")\n",
    "    print(f\"   Missing files: {missing_files}\")\n",
    "    print(f\"   Success rate: {successful_loads/len(sample_file_ids)*100:.1f}%\")\n",
    "    \n",
    "    if len(X) > 0:\n",
    "        print(f\"\\nDATASET STATISTICS:\")\n",
    "        print(f\"   Image shape: {X.shape}\")\n",
    "        print(f\"   Emotion range: [{y_emotions.min()}, {y_emotions.max()}]\")\n",
    "        print(f\"   Valence range: [{y_valence.min():.3f}, {y_valence.max():.3f}]\")\n",
    "        print(f\"   Arousal range: [{y_arousal.min():.3f}, {y_arousal.max():.3f}]\")\n",
    "        print(f\"   Ready for training!\")\n",
    "    else:\n",
    "        print(f\"\\nLOADING FAILED: No valid samples loaded!\")\n",
    "        print(f\"   Check paths and file availability\")\n",
    "    \n",
    "    return X, y_emotions, y_valence, y_arousal, valid_ids\n",
    "\n",
    "# Get file IDs from existing exploration\n",
    "file_ids = sorted([f.split('_')[0] for f in os.listdir(ANNOTATIONS_PATH) if f.endswith('_exp.npy')])\n",
    "\n",
    "print(f\"Available file IDs: {len(file_ids)}\")\n",
    "print(f\"Ready for enhanced dataset loading with detailed status!\")\n",
    "\n",
    "# Load dataset optimized for training\n",
    "SAMPLE_SIZE = 1000  # Balanced sample size for 20 epoch training\n",
    "\n",
    "X, y_emotions, y_valence, y_arousal, valid_ids = load_and_preprocess_dataset_fast(\n",
    "    IMAGES_PATH, ANNOTATIONS_PATH, file_ids, sample_size=SAMPLE_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cb19e1",
   "metadata": {},
   "source": [
    "## 8. Training Pipeline and Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090cf0ce",
   "metadata": {},
   "source": [
    "## 9. Model Evaluation and Results Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6b7795",
   "metadata": {},
   "source": [
    "##  Complete Training Pipeline for All Models\n",
    "\n",
    "**Ready to train all CNN architectures!** \n",
    "\n",
    "The cell below contains the complete training setup for all models. **Uncomment and run it** when you're ready for full training:\n",
    "\n",
    "- **Dataset**: All 3,999 samples\n",
    "- **Models**: All 9 CNN architectures  \n",
    "- **Training**: 50 epochs with early stopping\n",
    "- **Evaluation**: Complete metrics comparison\n",
    "\n",
    "**⚠️ Expected Training Time: 15-25 hours on Kaggle GPUs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894ec8e9",
   "metadata": {},
   "source": [
    "## Efficient Model Screening Strategy\n",
    "\n",
    "To optimize training time and resource usage, we'll implement a two-phase approach:\n",
    "\n",
    "**Phase 1: Quick Screening (5 epochs each)**\n",
    "- Test 6 best CNN models for facial expression recognition\n",
    "- Identify top 3 performers based on validation metrics\n",
    "- Estimated time: 2-3 hours\n",
    "\n",
    "**Phase 2: Full Training (25-30 epochs)**\n",
    "- Train only the top 3 models with full epochs\n",
    "- Comprehensive evaluation and comparison\n",
    "- Estimated time: 3-4 hours\n",
    "\n",
    "**Total estimated time: 5-7 hours (vs 15-25 hours for all models)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5166771a",
   "metadata": {},
   "source": [
    "## Phase 2: Full Training of Top Models\n",
    "\n",
    "Now we'll train the top 3 performing models with full epochs for comprehensive evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c505b0",
   "metadata": {},
   "source": [
    "## Final Results Summary and Comparison\n",
    "\n",
    "Comprehensive comparison of the top performing models with detailed metrics and recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd87df2",
   "metadata": {},
   "source": [
    "## Full Training Pipeline - All Models (20 Epochs)\n",
    "\n",
    "With Tesla P100 confirmed, we'll now train all 9 CNN architectures with optimized settings:\n",
    "- **20 epochs** with early stopping\n",
    "- **Fine-tuned hyperparameters** for P100\n",
    "- **Advanced callbacks** for optimal training\n",
    "- **Memory optimization** for 16GB GPU\n",
    "- **Comprehensive evaluation** for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d8c1a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model configuration updated: ResNet50 and EfficientNetB1 only\n",
      "Evaluation function ready\n"
     ]
    }
   ],
   "source": [
    "# MODEL CONFIGURATION\n",
    "MODEL_ARCHITECTURES = ['ResNet50', 'EfficientNetB1']\n",
    "\n",
    "# COMPREHENSIVE MODEL EVALUATION FUNCTION\n",
    "def evaluate_model_comprehensive(model, X_test, y_test_dict, model_name):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation of multi-task model\n",
    "    \"\"\"\n",
    "    print(f\"Evaluating {model_name}...\")\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict(X_test, batch_size=32, verbose=0)\n",
    "    \n",
    "    # Extract predictions for each task\n",
    "    y_emotion_pred = predictions[0]  # Emotion classification logits\n",
    "    y_valence_pred = predictions[1].flatten()  # Valence regression\n",
    "    y_arousal_pred = predictions[2].flatten()  # Arousal regression\n",
    "    \n",
    "    # Ground truth\n",
    "    y_emotion_true = y_test_dict['emotion_output']\n",
    "    y_valence_true = y_test_dict['valence_output']\n",
    "    y_arousal_true = y_test_dict['arousal_output']\n",
    "    \n",
    "    # Convert emotion predictions to class predictions\n",
    "    y_emotion_pred_classes = np.argmax(y_emotion_pred, axis=1)\n",
    "    y_emotion_true_classes = np.argmax(y_emotion_true, axis=1)\n",
    "    \n",
    "    # Evaluate emotion classification\n",
    "    emotion_metrics = evaluate_classification_metrics(\n",
    "        y_emotion_true_classes, y_emotion_pred_classes, y_emotion_pred\n",
    "    )\n",
    "    \n",
    "    # Evaluate valence regression\n",
    "    valence_metrics = evaluate_regression_metrics(y_valence_true, y_valence_pred)\n",
    "    \n",
    "    # Evaluate arousal regression  \n",
    "    arousal_metrics = evaluate_regression_metrics(y_arousal_true, y_arousal_pred)\n",
    "    \n",
    "    results = {\n",
    "        'emotion_metrics': emotion_metrics,\n",
    "        'valence_metrics': valence_metrics,\n",
    "        'arousal_metrics': arousal_metrics\n",
    "    }\n",
    "    \n",
    "    print(f\"  Emotion Accuracy: {emotion_metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Valence RMSE: {valence_metrics['rmse']:.4f}\")\n",
    "    print(f\"  Arousal RMSE: {arousal_metrics['rmse']:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Model configuration updated: ResNet50 and EfficientNetB1 only\")\n",
    "print(\"Evaluation function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "759fe6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean training pipeline ready\n",
      "Models to train: ['ResNet50', 'EfficientNetB1']\n"
     ]
    }
   ],
   "source": [
    "# CLEAN TRAINING PIPELINE - RESNET50 AND EFFICIENTNETB1 ONLY\n",
    "import time\n",
    "\n",
    "def train_model_clean(model_name):\n",
    "    \"\"\"\n",
    "    Clean training function for a single model\n",
    "    \"\"\"\n",
    "    print(f\"Training {model_name}...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Create model\n",
    "        model = create_multitask_model(model_name)\n",
    "        model = compile_multitask_model(model)\n",
    "        \n",
    "        # Training callbacks\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=EARLY_STOPPING_PATIENCE,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            ),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=REDUCE_LR_PATIENCE,\n",
    "                min_lr=1e-7,\n",
    "                verbose=1\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=EPOCHS,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Evaluate model\n",
    "        results = evaluate_model_comprehensive(model, X_val, y_val, model_name)\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        results['training_time'] = training_time\n",
    "        results['epochs_completed'] = len(history.history['loss'])\n",
    "        \n",
    "        print(f\"Training completed in {training_time/60:.1f} minutes\")\n",
    "        print(f\"Epochs completed: {results['epochs_completed']}\")\n",
    "        \n",
    "        return model, results, history\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error training {model_name}: {str(e)}\")\n",
    "        return None, {'error': str(e)}, None\n",
    "\n",
    "def train_both_models():\n",
    "    \"\"\"\n",
    "    Train both ResNet50 and EfficientNetB1 models\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"TRAINING RESNET50 AND EFFICIENTNETB1\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Check data availability\n",
    "    if not all(var in globals() for var in ['X_train', 'X_val', 'y_train', 'y_val']):\n",
    "        print(\"Error: Training data not available. Please run data loading first.\")\n",
    "        return None, None\n",
    "    \n",
    "    print(f\"Training samples: {X_train.shape[0]}\")\n",
    "    print(f\"Validation samples: {X_val.shape[0]}\")\n",
    "    print(f\"Epochs per model: {EPOCHS}\")\n",
    "    \n",
    "    results = {}\n",
    "    models = {}\n",
    "    histories = {}\n",
    "    \n",
    "    # Train both models\n",
    "    for model_name in MODEL_ARCHITECTURES:\n",
    "        print(f\"\\nTraining {model_name}...\")\n",
    "        model, result, history = train_model_clean(model_name)\n",
    "        \n",
    "        if model is not None:\n",
    "            models[model_name] = model\n",
    "        results[model_name] = result\n",
    "        if history is not None:\n",
    "            histories[model_name] = history\n",
    "    \n",
    "    return models, results, histories\n",
    "\n",
    "print(\"Clean training pipeline ready\")\n",
    "print(f\"Models to train: {MODEL_ARCHITECTURES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91f21775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results analysis function ready\n"
     ]
    }
   ],
   "source": [
    "# RESULTS ANALYSIS FUNCTION\n",
    "def analyze_results(results):\n",
    "    \"\"\"\n",
    "    Simple results analysis without emojis\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"TRAINING RESULTS ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    successful_models = []\n",
    "    failed_models = []\n",
    "    \n",
    "    for model_name, result in results.items():\n",
    "        if 'error' in result:\n",
    "            failed_models.append(model_name)\n",
    "        else:\n",
    "            successful_models.append(model_name)\n",
    "    \n",
    "    print(f\"Successful models: {len(successful_models)}/2\")\n",
    "    print(f\"Failed models: {len(failed_models)}/2\")\n",
    "    \n",
    "    if successful_models:\n",
    "        print(\"\\nPERFORMACE COMPARISON:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for model_name in successful_models:\n",
    "            result = results[model_name]\n",
    "            print(f\"\\n{model_name}:\")\n",
    "            print(f\"  Emotion Accuracy: {result['emotion_metrics']['accuracy']:.4f}\")\n",
    "            print(f\"  Valence RMSE: {result['valence_metrics']['rmse']:.4f}\")\n",
    "            print(f\"  Arousal RMSE: {result['arousal_metrics']['rmse']:.4f}\")\n",
    "            print(f\"  Training Time: {result['training_time']/60:.1f} minutes\")\n",
    "            print(f\"  Epochs: {result['epochs_completed']}\")\n",
    "        \n",
    "        # Find best model\n",
    "        best_emotion = max(successful_models, \n",
    "                          key=lambda x: results[x]['emotion_metrics']['accuracy'])\n",
    "        best_valence = min(successful_models, \n",
    "                          key=lambda x: results[x]['valence_metrics']['rmse'])\n",
    "        best_arousal = min(successful_models, \n",
    "                          key=lambda x: results[x]['arousal_metrics']['rmse'])\n",
    "        \n",
    "        print(\"\\nBEST PERFORMERS:\")\n",
    "        print(\"-\" * 20)\n",
    "        print(f\"Best Emotion: {best_emotion}\")\n",
    "        print(f\"Best Valence: {best_valence}\")  \n",
    "        print(f\"Best Arousal: {best_arousal}\")\n",
    "        \n",
    "        # Overall recommendation\n",
    "        emotion_scores = {name: results[name]['emotion_metrics']['accuracy'] \n",
    "                         for name in successful_models}\n",
    "        overall_best = max(emotion_scores.keys(), key=lambda x: emotion_scores[x])\n",
    "        \n",
    "        print(f\"\\nRECOMMENDED MODEL: {overall_best}\")\n",
    "        print(f\"Emotion Accuracy: {emotion_scores[overall_best]:.4f}\")\n",
    "    \n",
    "    if failed_models:\n",
    "        print(f\"\\nFailed models: {failed_models}\")\n",
    "        for model_name in failed_models:\n",
    "            print(f\"  {model_name}: {results[model_name]['error']}\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "\n",
    "print(\"Results analysis function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0538cb",
   "metadata": {},
   "source": [
    "# Training Instructions\n",
    "\n",
    "## Quick Start\n",
    "1. Run all cells in order to set up the environment and load data\n",
    "2. Use `train_both_models()` to train both ResNet50 and EfficientNetB1\n",
    "3. Use `analyze_results(results)` to view performance comparison\n",
    "\n",
    "## Example Usage\n",
    "```python\n",
    "# Train both models\n",
    "trained_models, results, histories = train_both_models()\n",
    "\n",
    "# Analyze results\n",
    "analyze_results(results)\n",
    "```\n",
    "\n",
    "The notebook now focuses on the two best-performing architectures with clean, professional code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82c6552a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for ResNet50 and EfficientNetB1...\n",
      "============================================================\n",
      "Dataset loaded: (1000, 224, 224, 3)\n",
      "Emotions: (1000,)\n",
      "Valence: (1000,)\n",
      "Arousal: (1000,)\n",
      "Training samples: 800\n",
      "Validation samples: 200\n",
      "Emotion classes: 8\n",
      "============================================================\n",
      "TRAINING RESNET50 AND EFFICIENTNETB1\n",
      "============================================================\n",
      "Training samples: 800\n",
      "Validation samples: 200\n",
      "Epochs per model: 50\n",
      "\n",
      "Training ResNet50...\n",
      "Training ResNet50...\n",
      "--------------------------------------------------\n",
      "Training samples: 800\n",
      "Validation samples: 200\n",
      "Emotion classes: 8\n",
      "============================================================\n",
      "TRAINING RESNET50 AND EFFICIENTNETB1\n",
      "============================================================\n",
      "Training samples: 800\n",
      "Validation samples: 200\n",
      "Epochs per model: 50\n",
      "\n",
      "Training ResNet50...\n",
      "Training ResNet50...\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1758828603.271718      78 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n",
      "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n",
      "Epoch 1/50\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1758828634.244551     110 service.cc:148] XLA service 0x796f3c003960 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1758828634.245365     110 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
      "I0000 00:00:1758828636.699408     110 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "I0000 00:00:1758828636.699408     110 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 3/50\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - arousal_output_loss: 0.4795 - arousal_output_mae: 0.5654 - emotion_output_accuracy: 0.0660 - emotion_output_loss: 2.4028 - loss: 3.3967 - valence_output_loss: 0.5145 - valence_output_mae: 0.5949"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1758828646.734248     110 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 240ms/step - arousal_output_loss: 0.3118 - arousal_output_mae: 0.4456 - emotion_output_accuracy: 0.1128 - emotion_output_loss: 2.2852 - loss: 2.9940 - valence_output_loss: 0.3971 - valence_output_mae: 0.5051 - val_arousal_output_loss: 0.5468 - val_arousal_output_mae: 0.6408 - val_emotion_output_accuracy: 0.1250 - val_emotion_output_loss: 2.7350 - val_loss: 4.1191 - val_valence_output_loss: 0.8169 - val_valence_output_mae: 0.7861 - learning_rate: 0.0010\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 240ms/step - arousal_output_loss: 0.3118 - arousal_output_mae: 0.4456 - emotion_output_accuracy: 0.1128 - emotion_output_loss: 2.2852 - loss: 2.9940 - valence_output_loss: 0.3971 - valence_output_mae: 0.5051 - val_arousal_output_loss: 0.5468 - val_arousal_output_mae: 0.6408 - val_emotion_output_accuracy: 0.1250 - val_emotion_output_loss: 2.7350 - val_loss: 4.1191 - val_valence_output_loss: 0.8169 - val_valence_output_mae: 0.7861 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "Epoch 2/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - arousal_output_loss: 0.1506 - arousal_output_mae: 0.3297 - emotion_output_accuracy: 0.1194 - emotion_output_loss: 2.0896 - loss: 2.4874 - valence_output_loss: 0.2472 - valence_output_mae: 0.4090 - val_arousal_output_loss: 0.4829 - val_arousal_output_mae: 0.5878 - val_emotion_output_accuracy: 0.1150 - val_emotion_output_loss: 2.2397 - val_loss: 2.9513 - val_valence_output_loss: 0.2075 - val_valence_output_mae: 0.3820 - learning_rate: 0.0010\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - arousal_output_loss: 0.1506 - arousal_output_mae: 0.3297 - emotion_output_accuracy: 0.1194 - emotion_output_loss: 2.0896 - loss: 2.4874 - valence_output_loss: 0.2472 - valence_output_mae: 0.4090 - val_arousal_output_loss: 0.4829 - val_arousal_output_mae: 0.5878 - val_emotion_output_accuracy: 0.1150 - val_emotion_output_loss: 2.2397 - val_loss: 2.9513 - val_valence_output_loss: 0.2075 - val_valence_output_mae: 0.3820 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "Epoch 3/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - arousal_output_loss: 0.1481 - arousal_output_mae: 0.3295 - emotion_output_accuracy: 0.1171 - emotion_output_loss: 2.0867 - loss: 2.4760 - valence_output_loss: 0.2411 - valence_output_mae: 0.4053 - val_arousal_output_loss: 0.1550 - val_arousal_output_mae: 0.3337 - val_emotion_output_accuracy: 0.1250 - val_emotion_output_loss: 2.0862 - val_loss: 2.4574 - val_valence_output_loss: 0.2109 - val_valence_output_mae: 0.3796 - learning_rate: 0.0010\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - arousal_output_loss: 0.1481 - arousal_output_mae: 0.3295 - emotion_output_accuracy: 0.1171 - emotion_output_loss: 2.0867 - loss: 2.4760 - valence_output_loss: 0.2411 - valence_output_mae: 0.4053 - val_arousal_output_loss: 0.1550 - val_arousal_output_mae: 0.3337 - val_emotion_output_accuracy: 0.1250 - val_emotion_output_loss: 2.0862 - val_loss: 2.4574 - val_valence_output_loss: 0.2109 - val_valence_output_mae: 0.3796 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "Epoch 4/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - arousal_output_loss: 0.1430 - arousal_output_mae: 0.3286 - emotion_output_accuracy: 0.1205 - emotion_output_loss: 2.0803 - loss: 2.4625 - valence_output_loss: 0.2391 - valence_output_mae: 0.4027 - val_arousal_output_loss: 0.1532 - val_arousal_output_mae: 0.3489 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0785 - val_loss: 2.4442 - val_valence_output_loss: 0.2085 - val_valence_output_mae: 0.3838 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - arousal_output_loss: 0.1430 - arousal_output_mae: 0.3286 - emotion_output_accuracy: 0.1205 - emotion_output_loss: 2.0803 - loss: 2.4625 - valence_output_loss: 0.2391 - valence_output_mae: 0.4027 - val_arousal_output_loss: 0.1532 - val_arousal_output_mae: 0.3489 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0785 - val_loss: 2.4442 - val_valence_output_loss: 0.2085 - val_valence_output_mae: 0.3838 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - arousal_output_loss: 0.1365 - arousal_output_mae: 0.3171 - emotion_output_accuracy: 0.1178 - emotion_output_loss: 2.0782 - loss: 2.4513 - valence_output_loss: 0.2366 - valence_output_mae: 0.3990 - val_arousal_output_loss: 0.1517 - val_arousal_output_mae: 0.3457 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0779 - val_loss: 2.4421 - val_valence_output_loss: 0.2081 - val_valence_output_mae: 0.3831 - learning_rate: 0.0010\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - arousal_output_loss: 0.1365 - arousal_output_mae: 0.3171 - emotion_output_accuracy: 0.1178 - emotion_output_loss: 2.0782 - loss: 2.4513 - valence_output_loss: 0.2366 - valence_output_mae: 0.3990 - val_arousal_output_loss: 0.1517 - val_arousal_output_mae: 0.3457 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0779 - val_loss: 2.4421 - val_valence_output_loss: 0.2081 - val_valence_output_mae: 0.3831 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "Epoch 6/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - arousal_output_loss: 0.1369 - arousal_output_mae: 0.3180 - emotion_output_accuracy: 0.1529 - emotion_output_loss: 2.0756 - loss: 2.4500 - valence_output_loss: 0.2374 - valence_output_mae: 0.4006 - val_arousal_output_loss: 0.1519 - val_arousal_output_mae: 0.3463 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0779 - val_loss: 2.4419 - val_valence_output_loss: 0.2075 - val_valence_output_mae: 0.3813 - learning_rate: 0.0010\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - arousal_output_loss: 0.1369 - arousal_output_mae: 0.3180 - emotion_output_accuracy: 0.1529 - emotion_output_loss: 2.0756 - loss: 2.4500 - valence_output_loss: 0.2374 - valence_output_mae: 0.4006 - val_arousal_output_loss: 0.1519 - val_arousal_output_mae: 0.3463 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0779 - val_loss: 2.4419 - val_valence_output_loss: 0.2075 - val_valence_output_mae: 0.3813 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "Epoch 7/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - arousal_output_loss: 0.1366 - arousal_output_mae: 0.3175 - emotion_output_accuracy: 0.1291 - emotion_output_loss: 2.0784 - loss: 2.4490 - valence_output_loss: 0.2340 - valence_output_mae: 0.3974 - val_arousal_output_loss: 0.1513 - val_arousal_output_mae: 0.3446 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0778 - val_loss: 2.4415 - val_valence_output_loss: 0.2075 - val_valence_output_mae: 0.3818 - learning_rate: 0.0010\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - arousal_output_loss: 0.1366 - arousal_output_mae: 0.3175 - emotion_output_accuracy: 0.1291 - emotion_output_loss: 2.0784 - loss: 2.4490 - valence_output_loss: 0.2340 - valence_output_mae: 0.3974 - val_arousal_output_loss: 0.1513 - val_arousal_output_mae: 0.3446 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0778 - val_loss: 2.4415 - val_valence_output_loss: 0.2075 - val_valence_output_mae: 0.3818 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "Epoch 8/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 65ms/step - arousal_output_loss: 0.1358 - arousal_output_mae: 0.3155 - emotion_output_accuracy: 0.1293 - emotion_output_loss: 2.0779 - loss: 2.4514 - valence_output_loss: 0.2376 - valence_output_mae: 0.4005 - val_arousal_output_loss: 0.1514 - val_arousal_output_mae: 0.3450 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0778 - val_loss: 2.4415 - val_valence_output_loss: 0.2075 - val_valence_output_mae: 0.3818 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 65ms/step - arousal_output_loss: 0.1358 - arousal_output_mae: 0.3155 - emotion_output_accuracy: 0.1293 - emotion_output_loss: 2.0779 - loss: 2.4514 - valence_output_loss: 0.2376 - valence_output_mae: 0.4005 - val_arousal_output_loss: 0.1514 - val_arousal_output_mae: 0.3450 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0778 - val_loss: 2.4415 - val_valence_output_loss: 0.2075 - val_valence_output_mae: 0.3818 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 65ms/step - arousal_output_loss: 0.1374 - arousal_output_mae: 0.3186 - emotion_output_accuracy: 0.1304 - emotion_output_loss: 2.0780 - loss: 2.4519 - valence_output_loss: 0.2365 - valence_output_mae: 0.4000 - val_arousal_output_loss: 0.1514 - val_arousal_output_mae: 0.3449 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0777 - val_loss: 2.4415 - val_valence_output_loss: 0.2075 - val_valence_output_mae: 0.3818 - learning_rate: 0.0010\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 65ms/step - arousal_output_loss: 0.1374 - arousal_output_mae: 0.3186 - emotion_output_accuracy: 0.1304 - emotion_output_loss: 2.0780 - loss: 2.4519 - valence_output_loss: 0.2365 - valence_output_mae: 0.4000 - val_arousal_output_loss: 0.1514 - val_arousal_output_mae: 0.3449 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0777 - val_loss: 2.4415 - val_valence_output_loss: 0.2075 - val_valence_output_mae: 0.3818 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "Epoch 10/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step - arousal_output_loss: 0.1362 - arousal_output_mae: 0.3172 - emotion_output_accuracy: 0.1304 - emotion_output_loss: 2.0780 - loss: 2.4501 - valence_output_loss: 0.2358 - valence_output_mae: 0.3993 - val_arousal_output_loss: 0.1515 - val_arousal_output_mae: 0.3451 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0778 - val_loss: 2.4416 - val_valence_output_loss: 0.2075 - val_valence_output_mae: 0.3817 - learning_rate: 0.0010\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step - arousal_output_loss: 0.1362 - arousal_output_mae: 0.3172 - emotion_output_accuracy: 0.1304 - emotion_output_loss: 2.0780 - loss: 2.4501 - valence_output_loss: 0.2358 - valence_output_mae: 0.3993 - val_arousal_output_loss: 0.1515 - val_arousal_output_mae: 0.3451 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0778 - val_loss: 2.4416 - val_valence_output_loss: 0.2075 - val_valence_output_mae: 0.3817 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "Epoch 11/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - arousal_output_loss: 0.1352 - arousal_output_mae: 0.3160 - emotion_output_accuracy: 0.1303 - emotion_output_loss: 2.0787 - loss: 2.4484 - valence_output_loss: 0.2345 - valence_output_mae: 0.3981\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 65ms/step - arousal_output_loss: 0.1354 - arousal_output_mae: 0.3162 - emotion_output_accuracy: 0.1304 - emotion_output_loss: 2.0788 - loss: 2.4484 - valence_output_loss: 0.2343 - valence_output_mae: 0.3978 - val_arousal_output_loss: 0.1513 - val_arousal_output_mae: 0.3445 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0779 - val_loss: 2.4415 - val_valence_output_loss: 0.2075 - val_valence_output_mae: 0.3818 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 65ms/step - arousal_output_loss: 0.1354 - arousal_output_mae: 0.3162 - emotion_output_accuracy: 0.1304 - emotion_output_loss: 2.0788 - loss: 2.4484 - valence_output_loss: 0.2343 - valence_output_mae: 0.3978 - val_arousal_output_loss: 0.1513 - val_arousal_output_mae: 0.3445 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0779 - val_loss: 2.4415 - val_valence_output_loss: 0.2075 - val_valence_output_mae: 0.3818 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 65ms/step - arousal_output_loss: 0.1359 - arousal_output_mae: 0.3170 - emotion_output_accuracy: 0.1304 - emotion_output_loss: 2.0778 - loss: 2.4497 - valence_output_loss: 0.2361 - valence_output_mae: 0.3993 - val_arousal_output_loss: 0.1512 - val_arousal_output_mae: 0.3441 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0779 - val_loss: 2.4415 - val_valence_output_loss: 0.2077 - val_valence_output_mae: 0.3822 - learning_rate: 5.0000e-04\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 65ms/step - arousal_output_loss: 0.1359 - arousal_output_mae: 0.3170 - emotion_output_accuracy: 0.1304 - emotion_output_loss: 2.0778 - loss: 2.4497 - valence_output_loss: 0.2361 - valence_output_mae: 0.3993 - val_arousal_output_loss: 0.1512 - val_arousal_output_mae: 0.3441 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0779 - val_loss: 2.4415 - val_valence_output_loss: 0.2077 - val_valence_output_mae: 0.3822 - learning_rate: 5.0000e-04\n",
      "Epoch 13/50\n",
      "Epoch 13/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step - arousal_output_loss: 0.1381 - arousal_output_mae: 0.3199 - emotion_output_accuracy: 0.1304 - emotion_output_loss: 2.0778 - loss: 2.4523 - valence_output_loss: 0.2364 - valence_output_mae: 0.3989 - val_arousal_output_loss: 0.1512 - val_arousal_output_mae: 0.3443 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0778 - val_loss: 2.4415 - val_valence_output_loss: 0.2076 - val_valence_output_mae: 0.3822 - learning_rate: 5.0000e-04\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step - arousal_output_loss: 0.1381 - arousal_output_mae: 0.3199 - emotion_output_accuracy: 0.1304 - emotion_output_loss: 2.0778 - loss: 2.4523 - valence_output_loss: 0.2364 - valence_output_mae: 0.3989 - val_arousal_output_loss: 0.1512 - val_arousal_output_mae: 0.3443 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0778 - val_loss: 2.4415 - val_valence_output_loss: 0.2076 - val_valence_output_mae: 0.3822 - learning_rate: 5.0000e-04\n",
      "Epoch 14/50\n",
      "Epoch 14/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - arousal_output_loss: 0.1355 - arousal_output_mae: 0.3169 - emotion_output_accuracy: 0.1304 - emotion_output_loss: 2.0777 - loss: 2.4485 - valence_output_loss: 0.2353 - valence_output_mae: 0.3986 - val_arousal_output_loss: 0.1510 - val_arousal_output_mae: 0.3435 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0779 - val_loss: 2.4414 - val_valence_output_loss: 0.2076 - val_valence_output_mae: 0.3821 - learning_rate: 5.0000e-04\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - arousal_output_loss: 0.1355 - arousal_output_mae: 0.3169 - emotion_output_accuracy: 0.1304 - emotion_output_loss: 2.0777 - loss: 2.4485 - valence_output_loss: 0.2353 - valence_output_mae: 0.3986 - val_arousal_output_loss: 0.1510 - val_arousal_output_mae: 0.3435 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0779 - val_loss: 2.4414 - val_valence_output_loss: 0.2076 - val_valence_output_mae: 0.3821 - learning_rate: 5.0000e-04\n",
      "Epoch 15/50\n",
      "Epoch 15/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - arousal_output_loss: 0.1341 - arousal_output_mae: 0.3143 - emotion_output_accuracy: 0.1303 - emotion_output_loss: 2.0776 - loss: 2.4473 - valence_output_loss: 0.2356 - valence_output_mae: 0.3986\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step - arousal_output_loss: 0.1344 - arousal_output_mae: 0.3146 - emotion_output_accuracy: 0.1304 - emotion_output_loss: 2.0777 - loss: 2.4473 - valence_output_loss: 0.2353 - valence_output_mae: 0.3983 - val_arousal_output_loss: 0.1511 - val_arousal_output_mae: 0.3441 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0778 - val_loss: 2.4414 - val_valence_output_loss: 0.2076 - val_valence_output_mae: 0.3821 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step - arousal_output_loss: 0.1344 - arousal_output_mae: 0.3146 - emotion_output_accuracy: 0.1304 - emotion_output_loss: 2.0777 - loss: 2.4473 - valence_output_loss: 0.2353 - valence_output_mae: 0.3983 - val_arousal_output_loss: 0.1511 - val_arousal_output_mae: 0.3441 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0778 - val_loss: 2.4414 - val_valence_output_loss: 0.2076 - val_valence_output_mae: 0.3821 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step - arousal_output_loss: 0.1359 - arousal_output_mae: 0.3168 - emotion_output_accuracy: 0.1304 - emotion_output_loss: 2.0766 - loss: 2.4478 - valence_output_loss: 0.2354 - valence_output_mae: 0.3980 - val_arousal_output_loss: 0.1511 - val_arousal_output_mae: 0.3441 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0778 - val_loss: 2.4414 - val_valence_output_loss: 0.2076 - val_valence_output_mae: 0.3822 - learning_rate: 2.5000e-04\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step - arousal_output_loss: 0.1359 - arousal_output_mae: 0.3168 - emotion_output_accuracy: 0.1304 - emotion_output_loss: 2.0766 - loss: 2.4478 - valence_output_loss: 0.2354 - valence_output_mae: 0.3980 - val_arousal_output_loss: 0.1511 - val_arousal_output_mae: 0.3441 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0778 - val_loss: 2.4414 - val_valence_output_loss: 0.2076 - val_valence_output_mae: 0.3822 - learning_rate: 2.5000e-04\n",
      "Epoch 17/50\n",
      "Epoch 17/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step - arousal_output_loss: 0.1347 - arousal_output_mae: 0.3155 - emotion_output_accuracy: 0.1304 - emotion_output_loss: 2.0773 - loss: 2.4466 - valence_output_loss: 0.2346 - valence_output_mae: 0.3982 - val_arousal_output_loss: 0.1511 - val_arousal_output_mae: 0.3440 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0778 - val_loss: 2.4415 - val_valence_output_loss: 0.2077 - val_valence_output_mae: 0.3823 - learning_rate: 2.5000e-04\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step - arousal_output_loss: 0.1347 - arousal_output_mae: 0.3155 - emotion_output_accuracy: 0.1304 - emotion_output_loss: 2.0773 - loss: 2.4466 - valence_output_loss: 0.2346 - valence_output_mae: 0.3982 - val_arousal_output_loss: 0.1511 - val_arousal_output_mae: 0.3440 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0778 - val_loss: 2.4415 - val_valence_output_loss: 0.2077 - val_valence_output_mae: 0.3823 - learning_rate: 2.5000e-04\n",
      "Epoch 18/50\n",
      "Epoch 18/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - arousal_output_loss: 0.1354 - arousal_output_mae: 0.3158 - emotion_output_accuracy: 0.1304 - emotion_output_loss: 2.0773 - loss: 2.4471 - valence_output_loss: 0.2344 - valence_output_mae: 0.3975 - val_arousal_output_loss: 0.1511 - val_arousal_output_mae: 0.3440 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0778 - val_loss: 2.4415 - val_valence_output_loss: 0.2077 - val_valence_output_mae: 0.3823 - learning_rate: 2.5000e-04\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - arousal_output_loss: 0.1354 - arousal_output_mae: 0.3158 - emotion_output_accuracy: 0.1304 - emotion_output_loss: 2.0773 - loss: 2.4471 - valence_output_loss: 0.2344 - valence_output_mae: 0.3975 - val_arousal_output_loss: 0.1511 - val_arousal_output_mae: 0.3440 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0778 - val_loss: 2.4415 - val_valence_output_loss: 0.2077 - val_valence_output_mae: 0.3823 - learning_rate: 2.5000e-04\n",
      "Epoch 19/50\n",
      "Epoch 19/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - arousal_output_loss: 0.1358 - arousal_output_mae: 0.3162 - emotion_output_accuracy: 0.1303 - emotion_output_loss: 2.0774 - loss: 2.4476 - valence_output_loss: 0.2345 - valence_output_mae: 0.3975\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step - arousal_output_loss: 0.1360 - arousal_output_mae: 0.3165 - emotion_output_accuracy: 0.1304 - emotion_output_loss: 2.0774 - loss: 2.4476 - valence_output_loss: 0.2342 - valence_output_mae: 0.3973 - val_arousal_output_loss: 0.1511 - val_arousal_output_mae: 0.3441 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0778 - val_loss: 2.4415 - val_valence_output_loss: 0.2077 - val_valence_output_mae: 0.3823 - learning_rate: 2.5000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step - arousal_output_loss: 0.1360 - arousal_output_mae: 0.3165 - emotion_output_accuracy: 0.1304 - emotion_output_loss: 2.0774 - loss: 2.4476 - valence_output_loss: 0.2342 - valence_output_mae: 0.3973 - val_arousal_output_loss: 0.1511 - val_arousal_output_mae: 0.3441 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0778 - val_loss: 2.4415 - val_valence_output_loss: 0.2077 - val_valence_output_mae: 0.3823 - learning_rate: 2.5000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - arousal_output_loss: 0.1353 - arousal_output_mae: 0.3167 - emotion_output_accuracy: 0.1304 - emotion_output_loss: 2.0770 - loss: 2.4471 - valence_output_loss: 0.2349 - valence_output_mae: 0.3979 - val_arousal_output_loss: 0.1511 - val_arousal_output_mae: 0.3441 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0778 - val_loss: 2.4415 - val_valence_output_loss: 0.2077 - val_valence_output_mae: 0.3824 - learning_rate: 1.2500e-04\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - arousal_output_loss: 0.1353 - arousal_output_mae: 0.3167 - emotion_output_accuracy: 0.1304 - emotion_output_loss: 2.0770 - loss: 2.4471 - valence_output_loss: 0.2349 - valence_output_mae: 0.3979 - val_arousal_output_loss: 0.1511 - val_arousal_output_mae: 0.3441 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0778 - val_loss: 2.4415 - val_valence_output_loss: 0.2077 - val_valence_output_mae: 0.3824 - learning_rate: 1.2500e-04\n",
      "Epoch 21/50\n",
      "Epoch 21/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step - arousal_output_loss: 0.1365 - arousal_output_mae: 0.3180 - emotion_output_accuracy: 0.1304 - emotion_output_loss: 2.0777 - loss: 2.4488 - valence_output_loss: 0.2346 - valence_output_mae: 0.3986 - val_arousal_output_loss: 0.1511 - val_arousal_output_mae: 0.3439 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0778 - val_loss: 2.4415 - val_valence_output_loss: 0.2077 - val_valence_output_mae: 0.3824 - learning_rate: 1.2500e-04\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step - arousal_output_loss: 0.1365 - arousal_output_mae: 0.3180 - emotion_output_accuracy: 0.1304 - emotion_output_loss: 2.0777 - loss: 2.4488 - valence_output_loss: 0.2346 - valence_output_mae: 0.3986 - val_arousal_output_loss: 0.1511 - val_arousal_output_mae: 0.3439 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0778 - val_loss: 2.4415 - val_valence_output_loss: 0.2077 - val_valence_output_mae: 0.3824 - learning_rate: 1.2500e-04\n",
      "Epoch 22/50\n",
      "Epoch 22/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step - arousal_output_loss: 0.1369 - arousal_output_mae: 0.3182 - emotion_output_accuracy: 0.1304 - emotion_output_loss: 2.0775 - loss: 2.4495 - valence_output_loss: 0.2351 - valence_output_mae: 0.3977 - val_arousal_output_loss: 0.1511 - val_arousal_output_mae: 0.3440 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0778 - val_loss: 2.4415 - val_valence_output_loss: 0.2077 - val_valence_output_mae: 0.3824 - learning_rate: 1.2500e-04\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step - arousal_output_loss: 0.1369 - arousal_output_mae: 0.3182 - emotion_output_accuracy: 0.1304 - emotion_output_loss: 2.0775 - loss: 2.4495 - valence_output_loss: 0.2351 - valence_output_mae: 0.3977 - val_arousal_output_loss: 0.1511 - val_arousal_output_mae: 0.3440 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0778 - val_loss: 2.4415 - val_valence_output_loss: 0.2077 - val_valence_output_mae: 0.3824 - learning_rate: 1.2500e-04\n",
      "Epoch 23/50\n",
      "Epoch 23/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - arousal_output_loss: 0.1360 - arousal_output_mae: 0.3165 - emotion_output_accuracy: 0.1303 - emotion_output_loss: 2.0773 - loss: 2.4488 - valence_output_loss: 0.2355 - valence_output_mae: 0.3986\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step - arousal_output_loss: 0.1363 - arousal_output_mae: 0.3168 - emotion_output_accuracy: 0.1304 - emotion_output_loss: 2.0773 - loss: 2.4488 - valence_output_loss: 0.2352 - valence_output_mae: 0.3983 - val_arousal_output_loss: 0.1511 - val_arousal_output_mae: 0.3440 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0778 - val_loss: 2.4415 - val_valence_output_loss: 0.2078 - val_valence_output_mae: 0.3824 - learning_rate: 1.2500e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step - arousal_output_loss: 0.1363 - arousal_output_mae: 0.3168 - emotion_output_accuracy: 0.1304 - emotion_output_loss: 2.0773 - loss: 2.4488 - valence_output_loss: 0.2352 - valence_output_mae: 0.3983 - val_arousal_output_loss: 0.1511 - val_arousal_output_mae: 0.3440 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0778 - val_loss: 2.4415 - val_valence_output_loss: 0.2078 - val_valence_output_mae: 0.3824 - learning_rate: 1.2500e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step - arousal_output_loss: 0.1360 - arousal_output_mae: 0.3163 - emotion_output_accuracy: 0.1304 - emotion_output_loss: 2.0770 - loss: 2.4477 - valence_output_loss: 0.2347 - valence_output_mae: 0.3977 - val_arousal_output_loss: 0.1511 - val_arousal_output_mae: 0.3440 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0778 - val_loss: 2.4415 - val_valence_output_loss: 0.2078 - val_valence_output_mae: 0.3824 - learning_rate: 6.2500e-05\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step - arousal_output_loss: 0.1360 - arousal_output_mae: 0.3163 - emotion_output_accuracy: 0.1304 - emotion_output_loss: 2.0770 - loss: 2.4477 - valence_output_loss: 0.2347 - valence_output_mae: 0.3977 - val_arousal_output_loss: 0.1511 - val_arousal_output_mae: 0.3440 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0778 - val_loss: 2.4415 - val_valence_output_loss: 0.2078 - val_valence_output_mae: 0.3824 - learning_rate: 6.2500e-05\n",
      "Epoch 24: early stopping\n",
      "Restoring model weights from the end of the best epoch: 14.\n",
      "Epoch 24: early stopping\n",
      "Restoring model weights from the end of the best epoch: 14.\n",
      "Evaluating ResNet50...\n",
      "Evaluating ResNet50...\n",
      "  Emotion Accuracy: 0.1350\n",
      "  Valence RMSE: 0.4592\n",
      "  Arousal RMSE: 0.3897\n",
      "Training completed in 2.4 minutes\n",
      "Epochs completed: 24\n",
      "\n",
      "Training EfficientNetB1...\n",
      "Training EfficientNetB1...\n",
      "--------------------------------------------------\n",
      "  Emotion Accuracy: 0.1350\n",
      "  Valence RMSE: 0.4592\n",
      "  Arousal RMSE: 0.3897\n",
      "Training completed in 2.4 minutes\n",
      "Epochs completed: 24\n",
      "\n",
      "Training EfficientNetB1...\n",
      "Training EfficientNetB1...\n",
      "--------------------------------------------------\n",
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb1_notop.h5\n",
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb1_notop.h5\n",
      "\u001b[1m27018416/27018416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
      "\u001b[1m27018416/27018416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
      "Epoch 1/50\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1758828792.070846     109 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1758828792.279319     109 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1758828792.279319     109 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1758828792.598695     109 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1758828792.598695     109 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1758828792.806607     109 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1758828792.806607     109 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1758828793.292676     109 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1758828793.292676     109 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1758828793.537174     109 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1758828793.537174     109 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 359ms/step - arousal_output_loss: 0.2200 - arousal_output_mae: 0.3897 - emotion_output_accuracy: 0.1127 - emotion_output_loss: 2.1327 - loss: 2.7785 - valence_output_loss: 0.4258 - valence_output_mae: 0.5304 - val_arousal_output_loss: 0.1514 - val_arousal_output_mae: 0.3445 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0830 - val_loss: 2.4708 - val_valence_output_loss: 0.2282 - val_valence_output_mae: 0.3824 - learning_rate: 0.0010\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 359ms/step - arousal_output_loss: 0.2200 - arousal_output_mae: 0.3897 - emotion_output_accuracy: 0.1127 - emotion_output_loss: 2.1327 - loss: 2.7785 - valence_output_loss: 0.4258 - valence_output_mae: 0.5304 - val_arousal_output_loss: 0.1514 - val_arousal_output_mae: 0.3445 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0830 - val_loss: 2.4708 - val_valence_output_loss: 0.2282 - val_valence_output_mae: 0.3824 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "Epoch 2/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - arousal_output_loss: 0.1910 - arousal_output_mae: 0.3605 - emotion_output_accuracy: 0.1393 - emotion_output_loss: 2.1203 - loss: 2.6160 - valence_output_loss: 0.3047 - valence_output_mae: 0.4493 - val_arousal_output_loss: 0.1684 - val_arousal_output_mae: 0.3364 - val_emotion_output_accuracy: 0.1250 - val_emotion_output_loss: 2.1018 - val_loss: 2.5067 - val_valence_output_loss: 0.2294 - val_valence_output_mae: 0.3831 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - arousal_output_loss: 0.1910 - arousal_output_mae: 0.3605 - emotion_output_accuracy: 0.1393 - emotion_output_loss: 2.1203 - loss: 2.6160 - valence_output_loss: 0.3047 - valence_output_mae: 0.4493 - val_arousal_output_loss: 0.1684 - val_arousal_output_mae: 0.3364 - val_emotion_output_accuracy: 0.1250 - val_emotion_output_loss: 2.1018 - val_loss: 2.5067 - val_valence_output_loss: 0.2294 - val_valence_output_mae: 0.3831 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - arousal_output_loss: 0.1656 - arousal_output_mae: 0.3436 - emotion_output_accuracy: 0.1144 - emotion_output_loss: 2.1283 - loss: 2.5484 - valence_output_loss: 0.2545 - valence_output_mae: 0.4129 - val_arousal_output_loss: 0.1638 - val_arousal_output_mae: 0.3628 - val_emotion_output_accuracy: 0.1300 - val_emotion_output_loss: 2.1203 - val_loss: 2.4921 - val_valence_output_loss: 0.2076 - val_valence_output_mae: 0.3817 - learning_rate: 0.0010\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - arousal_output_loss: 0.1656 - arousal_output_mae: 0.3436 - emotion_output_accuracy: 0.1144 - emotion_output_loss: 2.1283 - loss: 2.5484 - valence_output_loss: 0.2545 - valence_output_mae: 0.4129 - val_arousal_output_loss: 0.1638 - val_arousal_output_mae: 0.3628 - val_emotion_output_accuracy: 0.1300 - val_emotion_output_loss: 2.1203 - val_loss: 2.4921 - val_valence_output_loss: 0.2076 - val_valence_output_mae: 0.3817 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "Epoch 4/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - arousal_output_loss: 0.1487 - arousal_output_mae: 0.3276 - emotion_output_accuracy: 0.1182 - emotion_output_loss: 2.1075 - loss: 2.5240 - valence_output_loss: 0.2679 - valence_output_mae: 0.4272 - val_arousal_output_loss: 0.1512 - val_arousal_output_mae: 0.3444 - val_emotion_output_accuracy: 0.1250 - val_emotion_output_loss: 2.1268 - val_loss: 2.4945 - val_valence_output_loss: 0.2141 - val_valence_output_mae: 0.3889 - learning_rate: 0.0010\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - arousal_output_loss: 0.1487 - arousal_output_mae: 0.3276 - emotion_output_accuracy: 0.1182 - emotion_output_loss: 2.1075 - loss: 2.5240 - valence_output_loss: 0.2679 - valence_output_mae: 0.4272 - val_arousal_output_loss: 0.1512 - val_arousal_output_mae: 0.3444 - val_emotion_output_accuracy: 0.1250 - val_emotion_output_loss: 2.1268 - val_loss: 2.4945 - val_valence_output_loss: 0.2141 - val_valence_output_mae: 0.3889 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "Epoch 5/50\n",
      "\u001b[1m49/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - arousal_output_loss: 0.1614 - arousal_output_mae: 0.3421 - emotion_output_accuracy: 0.1064 - emotion_output_loss: 2.1003 - loss: 2.5124 - valence_output_loss: 0.2507 - valence_output_mae: 0.4140\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - arousal_output_loss: 0.1616 - arousal_output_mae: 0.3424 - emotion_output_accuracy: 0.1071 - emotion_output_loss: 2.1000 - loss: 2.5117 - valence_output_loss: 0.2502 - valence_output_mae: 0.4134 - val_arousal_output_loss: 0.1514 - val_arousal_output_mae: 0.3451 - val_emotion_output_accuracy: 0.1150 - val_emotion_output_loss: 2.1021 - val_loss: 2.5300 - val_valence_output_loss: 0.2785 - val_valence_output_mae: 0.4506 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - arousal_output_loss: 0.1616 - arousal_output_mae: 0.3424 - emotion_output_accuracy: 0.1071 - emotion_output_loss: 2.1000 - loss: 2.5117 - valence_output_loss: 0.2502 - valence_output_mae: 0.4134 - val_arousal_output_loss: 0.1514 - val_arousal_output_mae: 0.3451 - val_emotion_output_accuracy: 0.1150 - val_emotion_output_loss: 2.1021 - val_loss: 2.5300 - val_valence_output_loss: 0.2785 - val_valence_output_mae: 0.4506 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step - arousal_output_loss: 0.1414 - arousal_output_mae: 0.3175 - emotion_output_accuracy: 0.1045 - emotion_output_loss: 2.1084 - loss: 2.4969 - valence_output_loss: 0.2470 - valence_output_mae: 0.4107 - val_arousal_output_loss: 0.1518 - val_arousal_output_mae: 0.3463 - val_emotion_output_accuracy: 0.1300 - val_emotion_output_loss: 2.0979 - val_loss: 2.4613 - val_valence_output_loss: 0.2077 - val_valence_output_mae: 0.3811 - learning_rate: 5.0000e-04\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step - arousal_output_loss: 0.1414 - arousal_output_mae: 0.3175 - emotion_output_accuracy: 0.1045 - emotion_output_loss: 2.1084 - loss: 2.4969 - valence_output_loss: 0.2470 - valence_output_mae: 0.4107 - val_arousal_output_loss: 0.1518 - val_arousal_output_mae: 0.3463 - val_emotion_output_accuracy: 0.1300 - val_emotion_output_loss: 2.0979 - val_loss: 2.4613 - val_valence_output_loss: 0.2077 - val_valence_output_mae: 0.3811 - learning_rate: 5.0000e-04\n",
      "Epoch 7/50\n",
      "Epoch 7/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - arousal_output_loss: 0.1466 - arousal_output_mae: 0.3251 - emotion_output_accuracy: 0.1047 - emotion_output_loss: 2.0833 - loss: 2.4718 - valence_output_loss: 0.2419 - valence_output_mae: 0.4025 - val_arousal_output_loss: 0.1733 - val_arousal_output_mae: 0.3715 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0834 - val_loss: 2.4672 - val_valence_output_loss: 0.2081 - val_valence_output_mae: 0.3832 - learning_rate: 5.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - arousal_output_loss: 0.1466 - arousal_output_mae: 0.3251 - emotion_output_accuracy: 0.1047 - emotion_output_loss: 2.0833 - loss: 2.4718 - valence_output_loss: 0.2419 - valence_output_mae: 0.4025 - val_arousal_output_loss: 0.1733 - val_arousal_output_mae: 0.3715 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0834 - val_loss: 2.4672 - val_valence_output_loss: 0.2081 - val_valence_output_mae: 0.3832 - learning_rate: 5.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - arousal_output_loss: 0.1466 - arousal_output_mae: 0.3316 - emotion_output_accuracy: 0.1308 - emotion_output_loss: 2.0907 - loss: 2.4803 - valence_output_loss: 0.2429 - valence_output_mae: 0.4042 - val_arousal_output_loss: 0.1617 - val_arousal_output_mae: 0.3605 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0791 - val_loss: 2.4560 - val_valence_output_loss: 0.2125 - val_valence_output_mae: 0.3878 - learning_rate: 5.0000e-04\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - arousal_output_loss: 0.1466 - arousal_output_mae: 0.3316 - emotion_output_accuracy: 0.1308 - emotion_output_loss: 2.0907 - loss: 2.4803 - valence_output_loss: 0.2429 - valence_output_mae: 0.4042 - val_arousal_output_loss: 0.1617 - val_arousal_output_mae: 0.3605 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0791 - val_loss: 2.4560 - val_valence_output_loss: 0.2125 - val_valence_output_mae: 0.3878 - learning_rate: 5.0000e-04\n",
      "Epoch 9/50\n",
      "Epoch 9/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - arousal_output_loss: 0.1434 - arousal_output_mae: 0.3220 - emotion_output_accuracy: 0.1198 - emotion_output_loss: 2.0877 - loss: 2.4710 - valence_output_loss: 0.2399 - valence_output_mae: 0.4005 - val_arousal_output_loss: 0.1627 - val_arousal_output_mae: 0.3616 - val_emotion_output_accuracy: 0.1300 - val_emotion_output_loss: 2.0788 - val_loss: 2.4571 - val_valence_output_loss: 0.2139 - val_valence_output_mae: 0.3887 - learning_rate: 5.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - arousal_output_loss: 0.1434 - arousal_output_mae: 0.3220 - emotion_output_accuracy: 0.1198 - emotion_output_loss: 2.0877 - loss: 2.4710 - valence_output_loss: 0.2399 - valence_output_mae: 0.4005 - val_arousal_output_loss: 0.1627 - val_arousal_output_mae: 0.3616 - val_emotion_output_accuracy: 0.1300 - val_emotion_output_loss: 2.0788 - val_loss: 2.4571 - val_valence_output_loss: 0.2139 - val_valence_output_mae: 0.3887 - learning_rate: 5.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - arousal_output_loss: 0.1436 - arousal_output_mae: 0.3242 - emotion_output_accuracy: 0.1036 - emotion_output_loss: 2.0915 - loss: 2.4760 - valence_output_loss: 0.2409 - valence_output_mae: 0.4044 - val_arousal_output_loss: 0.1588 - val_arousal_output_mae: 0.3572 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0781 - val_loss: 2.4523 - val_valence_output_loss: 0.2127 - val_valence_output_mae: 0.3878 - learning_rate: 5.0000e-04\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - arousal_output_loss: 0.1436 - arousal_output_mae: 0.3242 - emotion_output_accuracy: 0.1036 - emotion_output_loss: 2.0915 - loss: 2.4760 - valence_output_loss: 0.2409 - valence_output_mae: 0.4044 - val_arousal_output_loss: 0.1588 - val_arousal_output_mae: 0.3572 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0781 - val_loss: 2.4523 - val_valence_output_loss: 0.2127 - val_valence_output_mae: 0.3878 - learning_rate: 5.0000e-04\n",
      "Epoch 11/50\n",
      "Epoch 11/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - arousal_output_loss: 0.1430 - arousal_output_mae: 0.3273 - emotion_output_accuracy: 0.1339 - emotion_output_loss: 2.0822 - loss: 2.4678 - valence_output_loss: 0.2427 - valence_output_mae: 0.4047 - val_arousal_output_loss: 0.1630 - val_arousal_output_mae: 0.3620 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0783 - val_loss: 2.4576 - val_valence_output_loss: 0.2140 - val_valence_output_mae: 0.3889 - learning_rate: 5.0000e-04\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - arousal_output_loss: 0.1430 - arousal_output_mae: 0.3273 - emotion_output_accuracy: 0.1339 - emotion_output_loss: 2.0822 - loss: 2.4678 - valence_output_loss: 0.2427 - valence_output_mae: 0.4047 - val_arousal_output_loss: 0.1630 - val_arousal_output_mae: 0.3620 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0783 - val_loss: 2.4576 - val_valence_output_loss: 0.2140 - val_valence_output_mae: 0.3889 - learning_rate: 5.0000e-04\n",
      "Epoch 12/50\n",
      "Epoch 12/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - arousal_output_loss: 0.1398 - arousal_output_mae: 0.3213 - emotion_output_accuracy: 0.1100 - emotion_output_loss: 2.0898 - loss: 2.4675 - valence_output_loss: 0.2379 - valence_output_mae: 0.4031 - val_arousal_output_loss: 0.1619 - val_arousal_output_mae: 0.3607 - val_emotion_output_accuracy: 0.1300 - val_emotion_output_loss: 2.0789 - val_loss: 2.4534 - val_valence_output_loss: 0.2105 - val_valence_output_mae: 0.3862 - learning_rate: 5.0000e-04\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - arousal_output_loss: 0.1398 - arousal_output_mae: 0.3213 - emotion_output_accuracy: 0.1100 - emotion_output_loss: 2.0898 - loss: 2.4675 - valence_output_loss: 0.2379 - valence_output_mae: 0.4031 - val_arousal_output_loss: 0.1619 - val_arousal_output_mae: 0.3607 - val_emotion_output_accuracy: 0.1300 - val_emotion_output_loss: 2.0789 - val_loss: 2.4534 - val_valence_output_loss: 0.2105 - val_valence_output_mae: 0.3862 - learning_rate: 5.0000e-04\n",
      "Epoch 13/50\n",
      "Epoch 13/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - arousal_output_loss: 0.1444 - arousal_output_mae: 0.3205 - emotion_output_accuracy: 0.1148 - emotion_output_loss: 2.0839 - loss: 2.4671 - valence_output_loss: 0.2388 - valence_output_mae: 0.3966 - val_arousal_output_loss: 0.1653 - val_arousal_output_mae: 0.3642 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0779 - val_loss: 2.4551 - val_valence_output_loss: 0.2095 - val_valence_output_mae: 0.3850 - learning_rate: 5.0000e-04\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - arousal_output_loss: 0.1444 - arousal_output_mae: 0.3205 - emotion_output_accuracy: 0.1148 - emotion_output_loss: 2.0839 - loss: 2.4671 - valence_output_loss: 0.2388 - valence_output_mae: 0.3966 - val_arousal_output_loss: 0.1653 - val_arousal_output_mae: 0.3642 - val_emotion_output_accuracy: 0.1350 - val_emotion_output_loss: 2.0779 - val_loss: 2.4551 - val_valence_output_loss: 0.2095 - val_valence_output_mae: 0.3850 - learning_rate: 5.0000e-04\n",
      "Epoch 14/50\n",
      "Epoch 14/50\n",
      "\u001b[1m49/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - arousal_output_loss: 0.1382 - arousal_output_mae: 0.3206 - emotion_output_accuracy: 0.1075 - emotion_output_loss: 2.0904 - loss: 2.4672 - valence_output_loss: 0.2386 - valence_output_mae: 0.4029\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - arousal_output_loss: 0.1386 - arousal_output_mae: 0.3211 - emotion_output_accuracy: 0.1077 - emotion_output_loss: 2.0905 - loss: 2.4672 - valence_output_loss: 0.2381 - valence_output_mae: 0.4023 - val_arousal_output_loss: 0.1757 - val_arousal_output_mae: 0.3735 - val_emotion_output_accuracy: 0.1250 - val_emotion_output_loss: 2.0785 - val_loss: 2.4711 - val_valence_output_loss: 0.2162 - val_valence_output_mae: 0.3903 - learning_rate: 5.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - arousal_output_loss: 0.1386 - arousal_output_mae: 0.3211 - emotion_output_accuracy: 0.1077 - emotion_output_loss: 2.0905 - loss: 2.4672 - valence_output_loss: 0.2381 - valence_output_mae: 0.4023 - val_arousal_output_loss: 0.1757 - val_arousal_output_mae: 0.3735 - val_emotion_output_accuracy: 0.1250 - val_emotion_output_loss: 2.0785 - val_loss: 2.4711 - val_valence_output_loss: 0.2162 - val_valence_output_mae: 0.3903 - learning_rate: 5.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - arousal_output_loss: 0.1477 - arousal_output_mae: 0.3299 - emotion_output_accuracy: 0.1416 - emotion_output_loss: 2.0831 - loss: 2.4676 - valence_output_loss: 0.2368 - valence_output_mae: 0.3999 - val_arousal_output_loss: 0.1578 - val_arousal_output_mae: 0.3557 - val_emotion_output_accuracy: 0.1300 - val_emotion_output_loss: 2.0785 - val_loss: 2.4474 - val_valence_output_loss: 0.2084 - val_valence_output_mae: 0.3836 - learning_rate: 2.5000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - arousal_output_loss: 0.1477 - arousal_output_mae: 0.3299 - emotion_output_accuracy: 0.1416 - emotion_output_loss: 2.0831 - loss: 2.4676 - valence_output_loss: 0.2368 - valence_output_mae: 0.3999 - val_arousal_output_loss: 0.1578 - val_arousal_output_mae: 0.3557 - val_emotion_output_accuracy: 0.1300 - val_emotion_output_loss: 2.0785 - val_loss: 2.4474 - val_valence_output_loss: 0.2084 - val_valence_output_mae: 0.3836 - learning_rate: 2.5000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - arousal_output_loss: 0.1433 - arousal_output_mae: 0.3256 - emotion_output_accuracy: 0.1511 - emotion_output_loss: 2.0751 - loss: 2.4582 - valence_output_loss: 0.2398 - valence_output_mae: 0.4033 - val_arousal_output_loss: 0.1553 - val_arousal_output_mae: 0.3522 - val_emotion_output_accuracy: 0.1300 - val_emotion_output_loss: 2.0785 - val_loss: 2.4457 - val_valence_output_loss: 0.2089 - val_valence_output_mae: 0.3844 - learning_rate: 2.5000e-04\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - arousal_output_loss: 0.1433 - arousal_output_mae: 0.3256 - emotion_output_accuracy: 0.1511 - emotion_output_loss: 2.0751 - loss: 2.4582 - valence_output_loss: 0.2398 - valence_output_mae: 0.4033 - val_arousal_output_loss: 0.1553 - val_arousal_output_mae: 0.3522 - val_emotion_output_accuracy: 0.1300 - val_emotion_output_loss: 2.0785 - val_loss: 2.4457 - val_valence_output_loss: 0.2089 - val_valence_output_mae: 0.3844 - learning_rate: 2.5000e-04\n",
      "Epoch 17/50\n",
      "Epoch 17/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - arousal_output_loss: 0.1411 - arousal_output_mae: 0.3216 - emotion_output_accuracy: 0.1300 - emotion_output_loss: 2.0832 - loss: 2.4610 - valence_output_loss: 0.2366 - valence_output_mae: 0.3991 - val_arousal_output_loss: 0.1626 - val_arousal_output_mae: 0.3615 - val_emotion_output_accuracy: 0.1300 - val_emotion_output_loss: 2.0788 - val_loss: 2.4545 - val_valence_output_loss: 0.2116 - val_valence_output_mae: 0.3870 - learning_rate: 2.5000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - arousal_output_loss: 0.1411 - arousal_output_mae: 0.3216 - emotion_output_accuracy: 0.1300 - emotion_output_loss: 2.0832 - loss: 2.4610 - valence_output_loss: 0.2366 - valence_output_mae: 0.3991 - val_arousal_output_loss: 0.1626 - val_arousal_output_mae: 0.3615 - val_emotion_output_accuracy: 0.1300 - val_emotion_output_loss: 2.0788 - val_loss: 2.4545 - val_valence_output_loss: 0.2116 - val_valence_output_mae: 0.3870 - learning_rate: 2.5000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - arousal_output_loss: 0.1417 - arousal_output_mae: 0.3217 - emotion_output_accuracy: 0.1152 - emotion_output_loss: 2.0770 - loss: 2.4522 - valence_output_loss: 0.2336 - valence_output_mae: 0.3970 - val_arousal_output_loss: 0.1570 - val_arousal_output_mae: 0.3548 - val_emotion_output_accuracy: 0.1250 - val_emotion_output_loss: 2.0790 - val_loss: 2.4495 - val_valence_output_loss: 0.2112 - val_valence_output_mae: 0.3867 - learning_rate: 2.5000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - arousal_output_loss: 0.1417 - arousal_output_mae: 0.3217 - emotion_output_accuracy: 0.1152 - emotion_output_loss: 2.0770 - loss: 2.4522 - valence_output_loss: 0.2336 - valence_output_mae: 0.3970 - val_arousal_output_loss: 0.1570 - val_arousal_output_mae: 0.3548 - val_emotion_output_accuracy: 0.1250 - val_emotion_output_loss: 2.0790 - val_loss: 2.4495 - val_valence_output_loss: 0.2112 - val_valence_output_mae: 0.3867 - learning_rate: 2.5000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - arousal_output_loss: 0.1388 - arousal_output_mae: 0.3200 - emotion_output_accuracy: 0.1110 - emotion_output_loss: 2.0811 - loss: 2.4543 - valence_output_loss: 0.2344 - valence_output_mae: 0.3972 - val_arousal_output_loss: 0.1567 - val_arousal_output_mae: 0.3544 - val_emotion_output_accuracy: 0.1300 - val_emotion_output_loss: 2.0781 - val_loss: 2.4480 - val_valence_output_loss: 0.2102 - val_valence_output_mae: 0.3858 - learning_rate: 2.5000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - arousal_output_loss: 0.1388 - arousal_output_mae: 0.3200 - emotion_output_accuracy: 0.1110 - emotion_output_loss: 2.0811 - loss: 2.4543 - valence_output_loss: 0.2344 - valence_output_mae: 0.3972 - val_arousal_output_loss: 0.1567 - val_arousal_output_mae: 0.3544 - val_emotion_output_accuracy: 0.1300 - val_emotion_output_loss: 2.0781 - val_loss: 2.4480 - val_valence_output_loss: 0.2102 - val_valence_output_mae: 0.3858 - learning_rate: 2.5000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m49/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - arousal_output_loss: 0.1357 - arousal_output_mae: 0.3169 - emotion_output_accuracy: 0.1289 - emotion_output_loss: 2.0851 - loss: 2.4550 - valence_output_loss: 0.2342 - valence_output_mae: 0.3961\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - arousal_output_loss: 0.1363 - arousal_output_mae: 0.3177 - emotion_output_accuracy: 0.1285 - emotion_output_loss: 2.0851 - loss: 2.4551 - valence_output_loss: 0.2337 - valence_output_mae: 0.3956 - val_arousal_output_loss: 0.1587 - val_arousal_output_mae: 0.3569 - val_emotion_output_accuracy: 0.1250 - val_emotion_output_loss: 2.0783 - val_loss: 2.4538 - val_valence_output_loss: 0.2143 - val_valence_output_mae: 0.3890 - learning_rate: 2.5000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - arousal_output_loss: 0.1363 - arousal_output_mae: 0.3177 - emotion_output_accuracy: 0.1285 - emotion_output_loss: 2.0851 - loss: 2.4551 - valence_output_loss: 0.2337 - valence_output_mae: 0.3956 - val_arousal_output_loss: 0.1587 - val_arousal_output_mae: 0.3569 - val_emotion_output_accuracy: 0.1250 - val_emotion_output_loss: 2.0783 - val_loss: 2.4538 - val_valence_output_loss: 0.2143 - val_valence_output_mae: 0.3890 - learning_rate: 2.5000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - arousal_output_loss: 0.1373 - arousal_output_mae: 0.3174 - emotion_output_accuracy: 0.1114 - emotion_output_loss: 2.0807 - loss: 2.4560 - valence_output_loss: 0.2380 - valence_output_mae: 0.4012 - val_arousal_output_loss: 0.1583 - val_arousal_output_mae: 0.3565 - val_emotion_output_accuracy: 0.1250 - val_emotion_output_loss: 2.0783 - val_loss: 2.4526 - val_valence_output_loss: 0.2135 - val_valence_output_mae: 0.3885 - learning_rate: 1.2500e-04\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - arousal_output_loss: 0.1373 - arousal_output_mae: 0.3174 - emotion_output_accuracy: 0.1114 - emotion_output_loss: 2.0807 - loss: 2.4560 - valence_output_loss: 0.2380 - valence_output_mae: 0.4012 - val_arousal_output_loss: 0.1583 - val_arousal_output_mae: 0.3565 - val_emotion_output_accuracy: 0.1250 - val_emotion_output_loss: 2.0783 - val_loss: 2.4526 - val_valence_output_loss: 0.2135 - val_valence_output_mae: 0.3885 - learning_rate: 1.2500e-04\n",
      "Epoch 22/50\n",
      "Epoch 22/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - arousal_output_loss: 0.1382 - arousal_output_mae: 0.3176 - emotion_output_accuracy: 0.1411 - emotion_output_loss: 2.0781 - loss: 2.4515 - valence_output_loss: 0.2352 - valence_output_mae: 0.3977 - val_arousal_output_loss: 0.1572 - val_arousal_output_mae: 0.3550 - val_emotion_output_accuracy: 0.1250 - val_emotion_output_loss: 2.0781 - val_loss: 2.4509 - val_valence_output_loss: 0.2129 - val_valence_output_mae: 0.3880 - learning_rate: 1.2500e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - arousal_output_loss: 0.1382 - arousal_output_mae: 0.3176 - emotion_output_accuracy: 0.1411 - emotion_output_loss: 2.0781 - loss: 2.4515 - valence_output_loss: 0.2352 - valence_output_mae: 0.3977 - val_arousal_output_loss: 0.1572 - val_arousal_output_mae: 0.3550 - val_emotion_output_accuracy: 0.1250 - val_emotion_output_loss: 2.0781 - val_loss: 2.4509 - val_valence_output_loss: 0.2129 - val_valence_output_mae: 0.3880 - learning_rate: 1.2500e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - arousal_output_loss: 0.1412 - arousal_output_mae: 0.3217 - emotion_output_accuracy: 0.1363 - emotion_output_loss: 2.0764 - loss: 2.4561 - valence_output_loss: 0.2385 - valence_output_mae: 0.4011 - val_arousal_output_loss: 0.1560 - val_arousal_output_mae: 0.3534 - val_emotion_output_accuracy: 0.1250 - val_emotion_output_loss: 2.0780 - val_loss: 2.4497 - val_valence_output_loss: 0.2127 - val_valence_output_mae: 0.3878 - learning_rate: 1.2500e-04\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - arousal_output_loss: 0.1412 - arousal_output_mae: 0.3217 - emotion_output_accuracy: 0.1363 - emotion_output_loss: 2.0764 - loss: 2.4561 - valence_output_loss: 0.2385 - valence_output_mae: 0.4011 - val_arousal_output_loss: 0.1560 - val_arousal_output_mae: 0.3534 - val_emotion_output_accuracy: 0.1250 - val_emotion_output_loss: 2.0780 - val_loss: 2.4497 - val_valence_output_loss: 0.2127 - val_valence_output_mae: 0.3878 - learning_rate: 1.2500e-04\n",
      "Epoch 24/50\n",
      "Epoch 24/50\n",
      "\u001b[1m49/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - arousal_output_loss: 0.1378 - arousal_output_mae: 0.3178 - emotion_output_accuracy: 0.1256 - emotion_output_loss: 2.0789 - loss: 2.4562 - valence_output_loss: 0.2395 - valence_output_mae: 0.4008\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - arousal_output_loss: 0.1382 - arousal_output_mae: 0.3185 - emotion_output_accuracy: 0.1261 - emotion_output_loss: 2.0789 - loss: 2.4560 - valence_output_loss: 0.2389 - valence_output_mae: 0.4003 - val_arousal_output_loss: 0.1567 - val_arousal_output_mae: 0.3544 - val_emotion_output_accuracy: 0.1250 - val_emotion_output_loss: 2.0779 - val_loss: 2.4499 - val_valence_output_loss: 0.2122 - val_valence_output_mae: 0.3875 - learning_rate: 1.2500e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - arousal_output_loss: 0.1382 - arousal_output_mae: 0.3185 - emotion_output_accuracy: 0.1261 - emotion_output_loss: 2.0789 - loss: 2.4560 - valence_output_loss: 0.2389 - valence_output_mae: 0.4003 - val_arousal_output_loss: 0.1567 - val_arousal_output_mae: 0.3544 - val_emotion_output_accuracy: 0.1250 - val_emotion_output_loss: 2.0779 - val_loss: 2.4499 - val_valence_output_loss: 0.2122 - val_valence_output_mae: 0.3875 - learning_rate: 1.2500e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - arousal_output_loss: 0.1392 - arousal_output_mae: 0.3191 - emotion_output_accuracy: 0.1206 - emotion_output_loss: 2.0804 - loss: 2.4563 - valence_output_loss: 0.2367 - valence_output_mae: 0.4010 - val_arousal_output_loss: 0.1559 - val_arousal_output_mae: 0.3531 - val_emotion_output_accuracy: 0.1250 - val_emotion_output_loss: 2.0780 - val_loss: 2.4495 - val_valence_output_loss: 0.2125 - val_valence_output_mae: 0.3877 - learning_rate: 6.2500e-05\n",
      "Epoch 26/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - arousal_output_loss: 0.1392 - arousal_output_mae: 0.3191 - emotion_output_accuracy: 0.1206 - emotion_output_loss: 2.0804 - loss: 2.4563 - valence_output_loss: 0.2367 - valence_output_mae: 0.4010 - val_arousal_output_loss: 0.1559 - val_arousal_output_mae: 0.3531 - val_emotion_output_accuracy: 0.1250 - val_emotion_output_loss: 2.0780 - val_loss: 2.4495 - val_valence_output_loss: 0.2125 - val_valence_output_mae: 0.3877 - learning_rate: 6.2500e-05\n",
      "Epoch 26/50\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - arousal_output_loss: 0.1369 - arousal_output_mae: 0.3185 - emotion_output_accuracy: 0.1035 - emotion_output_loss: 2.0803 - loss: 2.4528 - valence_output_loss: 0.2356 - valence_output_mae: 0.3961 - val_arousal_output_loss: 0.1563 - val_arousal_output_mae: 0.3538 - val_emotion_output_accuracy: 0.1250 - val_emotion_output_loss: 2.0781 - val_loss: 2.4500 - val_valence_output_loss: 0.2126 - val_valence_output_mae: 0.3877 - learning_rate: 6.2500e-05\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - arousal_output_loss: 0.1369 - arousal_output_mae: 0.3185 - emotion_output_accuracy: 0.1035 - emotion_output_loss: 2.0803 - loss: 2.4528 - valence_output_loss: 0.2356 - valence_output_mae: 0.3961 - val_arousal_output_loss: 0.1563 - val_arousal_output_mae: 0.3538 - val_emotion_output_accuracy: 0.1250 - val_emotion_output_loss: 2.0781 - val_loss: 2.4500 - val_valence_output_loss: 0.2126 - val_valence_output_mae: 0.3877 - learning_rate: 6.2500e-05\n",
      "Epoch 26: early stopping\n",
      "Restoring model weights from the end of the best epoch: 16.\n",
      "Epoch 26: early stopping\n",
      "Restoring model weights from the end of the best epoch: 16.\n",
      "Evaluating EfficientNetB1...\n",
      "Evaluating EfficientNetB1...\n",
      "  Emotion Accuracy: 0.1300\n",
      "  Valence RMSE: 0.4604\n",
      "  Arousal RMSE: 0.3937\n",
      "Training completed in 2.5 minutes\n",
      "Epochs completed: 26\n",
      "============================================================\n",
      "TRAINING RESULTS ANALYSIS\n",
      "============================================================\n",
      "Successful models: 2/2\n",
      "Failed models: 0/2\n",
      "\n",
      "PERFORMACE COMPARISON:\n",
      "----------------------------------------\n",
      "\n",
      "ResNet50:\n",
      "  Emotion Accuracy: 0.1350\n",
      "  Valence RMSE: 0.4592\n",
      "  Arousal RMSE: 0.3897\n",
      "  Training Time: 2.4 minutes\n",
      "  Epochs: 24\n",
      "\n",
      "EfficientNetB1:\n",
      "  Emotion Accuracy: 0.1300\n",
      "  Valence RMSE: 0.4604\n",
      "  Arousal RMSE: 0.3937\n",
      "  Training Time: 2.5 minutes\n",
      "  Epochs: 26\n",
      "\n",
      "BEST PERFORMERS:\n",
      "--------------------\n",
      "Best Emotion: ResNet50\n",
      "Best Valence: ResNet50\n",
      "Best Arousal: ResNet50\n",
      "\n",
      "RECOMMENDED MODEL: ResNet50\n",
      "Emotion Accuracy: 0.1350\n",
      "============================================================\n",
      "  Emotion Accuracy: 0.1300\n",
      "  Valence RMSE: 0.4604\n",
      "  Arousal RMSE: 0.3937\n",
      "Training completed in 2.5 minutes\n",
      "Epochs completed: 26\n",
      "============================================================\n",
      "TRAINING RESULTS ANALYSIS\n",
      "============================================================\n",
      "Successful models: 2/2\n",
      "Failed models: 0/2\n",
      "\n",
      "PERFORMACE COMPARISON:\n",
      "----------------------------------------\n",
      "\n",
      "ResNet50:\n",
      "  Emotion Accuracy: 0.1350\n",
      "  Valence RMSE: 0.4592\n",
      "  Arousal RMSE: 0.3897\n",
      "  Training Time: 2.4 minutes\n",
      "  Epochs: 24\n",
      "\n",
      "EfficientNetB1:\n",
      "  Emotion Accuracy: 0.1300\n",
      "  Valence RMSE: 0.4604\n",
      "  Arousal RMSE: 0.3937\n",
      "  Training Time: 2.5 minutes\n",
      "  Epochs: 26\n",
      "\n",
      "BEST PERFORMERS:\n",
      "--------------------\n",
      "Best Emotion: ResNet50\n",
      "Best Valence: ResNet50\n",
      "Best Arousal: ResNet50\n",
      "\n",
      "RECOMMENDED MODEL: ResNet50\n",
      "Emotion Accuracy: 0.1350\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Execute the training pipeline\n",
    "print(\"Starting training for ResNet50 and EfficientNetB1...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check if data is loaded\n",
    "if 'X' in globals():\n",
    "    print(f\"Dataset loaded: {X.shape}\")\n",
    "    print(f\"Emotions: {y_emotions.shape}\")\n",
    "    print(f\"Valence: {y_valence.shape}\")\n",
    "    print(f\"Arousal: {y_arousal.shape}\")\n",
    "    \n",
    "    # Split the data into training and validation sets\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_val, y_emotions_train, y_emotions_val = train_test_split(\n",
    "        X, y_emotions, test_size=VALIDATION_SPLIT, random_state=42, stratify=y_emotions\n",
    "    )\n",
    "    \n",
    "    _, _, y_valence_train, y_valence_val = train_test_split(\n",
    "        X, y_valence, test_size=VALIDATION_SPLIT, random_state=42, stratify=y_emotions\n",
    "    )\n",
    "    \n",
    "    _, _, y_arousal_train, y_arousal_val = train_test_split(\n",
    "        X, y_arousal, test_size=VALIDATION_SPLIT, random_state=42, stratify=y_emotions\n",
    "    )\n",
    "    \n",
    "    # Convert emotion labels to categorical (one-hot encoded)\n",
    "    from tensorflow.keras.utils import to_categorical\n",
    "    y_emotions_train_cat = to_categorical(y_emotions_train, NUM_CLASSES)\n",
    "    y_emotions_val_cat = to_categorical(y_emotions_val, NUM_CLASSES)\n",
    "    \n",
    "    # Prepare train and validation dictionaries with correct output names\n",
    "    y_train = {\n",
    "        'emotion_output': y_emotions_train_cat,\n",
    "        'valence_output': y_valence_train,\n",
    "        'arousal_output': y_arousal_train\n",
    "    }\n",
    "    \n",
    "    y_val = {\n",
    "        'emotion_output': y_emotions_val_cat,\n",
    "        'valence_output': y_valence_val,\n",
    "        'arousal_output': y_arousal_val\n",
    "    }\n",
    "    \n",
    "    print(f\"Training samples: {X_train.shape[0]}\")\n",
    "    print(f\"Validation samples: {X_val.shape[0]}\")\n",
    "    print(f\"Emotion classes: {y_emotions_train_cat.shape[1]}\")\n",
    "    \n",
    "    # Train both models\n",
    "    models, results, histories = train_both_models()\n",
    "    \n",
    "    # Analyze results\n",
    "    if results:\n",
    "        analyze_results(results)\n",
    "    else:\n",
    "        print(\"Training failed - no results to analyze\")\n",
    "        \n",
    "else:\n",
    "    print(\"Error: Dataset not loaded. Please run the data loading cell first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
